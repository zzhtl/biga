use crate::models::{ModelConfig, create_model, save_model};
use candle_core::{Device, Tensor, DType};
use candle_nn::{Module, Optimizer, VarBuilder, VarMap, AdamW};
use serde::{Deserialize, Serialize};
use std::path::{Path, PathBuf};
use uuid::Uuid;
use std::fs;
use std::time::{SystemTime, UNIX_EPOCH};
use rand;
use chrono::{self, Weekday, Datelike};
use sqlx::Row; // æ·»åŠ Row traitå¯¼å…¥
use std::sync::{Arc, Mutex, OnceLock};
use std::collections::VecDeque;

// æ•°æ®åº“è·¯å¾„æŸ¥æ‰¾å‡½æ•°
fn find_database_path() -> Option<PathBuf> {
    let current_dir = std::env::current_dir().ok()?;
    
    // å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®åº“è·¯å¾„
    let possible_paths = [
        current_dir.join("db/stock_data.db"),
        current_dir.join("src-tauri/db/stock_data.db"),
        current_dir.parent()?.join("src-tauri/db/stock_data.db"), // å¦‚æœåœ¨ src-tauri ç›®å½•å†…è¿è¡Œ
    ];
    
    for path in &possible_paths {
        println!("æ£€æŸ¥æ•°æ®åº“è·¯å¾„: {}", path.display());
        if path.exists() {
            println!("âœ… æ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶: {}", path.display());
            return Some(path.clone());
        }
    }
    
    println!("âŒ æœªæ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶");
    None
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelInfo {
    pub id: String,
    pub name: String, 
    pub stock_code: String,
    pub created_at: u64,
    pub model_type: String,
    pub features: Vec<String>,
    pub target: String,
    pub prediction_days: usize,
    pub accuracy: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Prediction {
    pub target_date: String,
    pub predicted_price: f64,
    pub predicted_change_percent: f64,
    pub confidence: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingRequest {
    pub stock_code: String,
    pub model_name: String,
    pub start_date: String,
    pub end_date: String,
    pub features: Vec<String>,
    pub target: String,
    pub prediction_days: usize,
    pub model_type: String,
    pub epochs: usize,
    pub batch_size: usize,
    pub learning_rate: f64,
    pub dropout: f64,
    pub train_test_split: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PredictionRequest {
    pub stock_code: String,
    pub model_name: Option<String>,
    pub prediction_days: usize,
    pub use_candle: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TrainingResult {
    pub metadata: ModelInfo,
    pub accuracy: f64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct EvaluationResult {
    pub accuracy: f64,
    pub confusion_matrix: Vec<Vec<usize>>,
    pub precision: f64,
    pub recall: f64,
    pub f1_score: f64,
    pub prediction_examples: Vec<PredictionExample>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PredictionExample {
    pub actual: f64,
    pub predicted: f64,
    pub features: Vec<f64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingLog {
    pub epoch: usize,
    pub loss: f64,
    pub timestamp: String,
    pub message: Option<String>,
}

// å…¨å±€è®­ç»ƒæ—¥å¿—å­˜å‚¨
static TRAINING_LOGS: OnceLock<Arc<Mutex<VecDeque<TrainingLog>>>> = OnceLock::new();

fn get_training_logs() -> Arc<Mutex<VecDeque<TrainingLog>>> {
    TRAINING_LOGS.get_or_init(|| Arc::new(Mutex::new(VecDeque::new()))).clone()
}

// å®šä¹‰è·å–æ¨¡å‹ä¿å­˜ç›®å½•çš„å‡½æ•°
fn get_models_dir() -> PathBuf {
    let app_dir = dirs::data_dir().unwrap_or_else(|| PathBuf::from("./data"));
    let models_dir = app_dir.join("biga/models");
    fs::create_dir_all(&models_dir).unwrap_or_default();
    models_dir
}

// å®šä¹‰è·å–ç‰¹å®šæ¨¡å‹ç›®å½•çš„å‡½æ•°
fn get_model_dir(model_id: &str) -> PathBuf {
    get_models_dir().join(model_id)
}

// ä¿å­˜æ¨¡å‹å…ƒæ•°æ®
fn save_model_metadata(metadata: &ModelInfo) -> std::io::Result<()> {
    let model_dir = get_model_dir(&metadata.id);
    fs::create_dir_all(&model_dir)?;
    
    let metadata_path = model_dir.join("metadata.json");
    let metadata_json = serde_json::to_string_pretty(metadata)?;
    fs::write(metadata_path, metadata_json)?;
    
    Ok(())
}

// è¯»å–æ¨¡å‹å…ƒæ•°æ®
fn load_model_metadata(model_id: &str) -> std::io::Result<ModelInfo> {
    let metadata_path = get_model_dir(model_id).join("metadata.json");
    let metadata_json = fs::read_to_string(metadata_path)?;
    let metadata: ModelInfo = serde_json::from_str(&metadata_json)?;
    Ok(metadata)
}

// åˆ—å‡ºç‰¹å®šè‚¡ç¥¨ä»£ç çš„æ‰€æœ‰æ¨¡å‹
pub fn list_models(symbol: &str) -> Vec<ModelInfo> {
    let models_dir = get_models_dir();
    
    let mut models = Vec::new();
    
    if let Ok(entries) = fs::read_dir(models_dir) {
        for entry in entries.flatten() {
            let path = entry.path();
            
            if path.is_dir() {
                let metadata_path = path.join("metadata.json");
                
                if metadata_path.exists() {
                    if let Ok(metadata_json) = fs::read_to_string(metadata_path) {
                        if let Ok(metadata) = serde_json::from_str::<ModelInfo>(&metadata_json) {
                            if metadata.stock_code == symbol {
                                models.push(metadata);
                            }
                        }
                    }
                }
            }
        }
    }
    
    // æŒ‰åˆ›å»ºæ—¶é—´é™åºæ’åº
    models.sort_by(|a, b| b.created_at.cmp(&a.created_at));
    models
}

// åˆ é™¤æ¨¡å‹
pub fn delete_model(model_id: &str) -> std::io::Result<()> {
    let model_dir = get_model_dir(model_id);
    fs::remove_dir_all(model_dir)?;
    Ok(())
}

// æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†å‡½æ•°
async fn prepare_stock_data(
    request: &TrainingRequest,
) -> std::result::Result<(Tensor, Tensor, Tensor, Tensor, Vec<String>), candle_core::Error> {
    // è®¾ç½®è®¾å¤‡
    let device = Device::Cpu;
    
    // æ‰©å±•æ•°æ®è·å–èŒƒå›´ä»¥æé«˜æ¨¡å‹å‡†ç¡®ç‡
    let symbol = &request.stock_code;
    
    // è‡ªåŠ¨è®¡ç®—æ›´é•¿çš„æ—¶é—´èŒƒå›´ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®
    let end_date = chrono::Local::now().naive_local().date();
    let extended_start_date = end_date - chrono::Duration::days(500); // æ‰©å±•åˆ°çº¦1.5å¹´
    
    // ä¼˜å…ˆä½¿ç”¨æ‰©å±•çš„æ—¶é—´èŒƒå›´ï¼Œå¦‚æœç”¨æˆ·æŒ‡å®šçš„èŒƒå›´æ›´å¤§åˆ™ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„
    let actual_start_date = if let Ok(user_start) = chrono::NaiveDate::parse_from_str(&request.start_date, "%Y-%m-%d") {
        if user_start < extended_start_date {
            user_start
        } else {
            extended_start_date
        }
    } else {
        extended_start_date
    };
    
    let actual_end_date = if let Ok(user_end) = chrono::NaiveDate::parse_from_str(&request.end_date, "%Y-%m-%d") {
        if user_end > end_date {
            end_date
        } else {
            user_end
        }
    } else {
        end_date
    };
    
    let start_date_str = actual_start_date.format("%Y-%m-%d").to_string();
    let end_date_str = actual_end_date.format("%Y-%m-%d").to_string();
    
    println!("ğŸš€ ä½¿ç”¨æ‰©å±•è®­ç»ƒæ•°æ®èŒƒå›´: {} åˆ° {} (çº¦{}å¤©)", 
             start_date_str, end_date_str, 
             (actual_end_date - actual_start_date).num_days());
    
    // ä½¿ç”¨sqlxæŸ¥è¯¢æ•°æ®åº“è·å–å†å²æ•°æ®
    let historical_data = match get_historical_data_from_db(symbol, &start_date_str, &end_date_str).await {
        Ok(data) => data,
        Err(e) => {
            eprintln!("ä»æ•°æ®åº“è·å–æ•°æ®å¤±è´¥: {}", e);
            return Err(candle_core::Error::Msg(format!("è·å–å†å²æ•°æ®å¤±è´¥: {}", e)));
        }
    };
    
    if historical_data.is_empty() {
        return Err(candle_core::Error::Msg("å†å²æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹".to_string()));
    }
    
    println!("âœ… è·å–åˆ°{}æ¡å†å²æ•°æ®", historical_data.len());
    
    // æ•°æ®è´¨é‡æ£€æŸ¥
    let valid_data: Vec<_> = historical_data.into_iter()
        .filter(|data| {
            data.close > 0.0 && data.volume >= 0 && 
            data.open > 0.0 && data.high > 0.0 && data.low > 0.0 &&
            data.high >= data.low && data.high >= data.open && 
            data.high >= data.close && data.low <= data.open && data.low <= data.close
        })
        .collect();
    
    println!("âœ… è¿‡æ»¤åæœ‰æ•ˆæ•°æ®{}æ¡", valid_data.len());
    
    // é™ä½æœ€å°æ•°æ®è¦æ±‚ï¼Œä½†å»ºè®®ä½¿ç”¨æ›´å¤šæ•°æ®
    let min_required_days = 60; // æœ€å°‘60å¤©
    let recommended_days = 200; // æ¨è200å¤©ä»¥ä¸Š
    
    if valid_data.len() < min_required_days {
        return Err(candle_core::Error::Msg(format!(
            "æœ‰æ•ˆå†å²æ•°æ®ä¸è¶³ï¼Œå½“å‰{}å¤©ï¼Œéœ€è¦è‡³å°‘{}å¤©æ•°æ®", 
            valid_data.len(), min_required_days
        )));
    }
    
    if valid_data.len() < recommended_days {
        println!("âš ï¸  è­¦å‘Š: å½“å‰æ•°æ®é‡{}å¤©å°‘äºæ¨èçš„{}å¤©ï¼Œå¯èƒ½å½±å“æ¨¡å‹å‡†ç¡®ç‡", 
                 valid_data.len(), recommended_days);
    }
    
    // æ„å»ºç‰¹å¾å’Œæ ‡ç­¾
    let mut dates = Vec::new();
    let mut prices = Vec::new();
    let mut volumes = Vec::new();
    let mut features_matrix = Vec::new();
    
    // æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
    let mut sorted_data = valid_data.clone();
    sorted_data.sort_by(|a, b| a.date.cmp(&b.date));
    
    // é¦–å…ˆæå–åŸºç¡€æ•°æ®
    for data in &sorted_data {
        dates.push(data.date.clone());
        prices.push(data.close);
        volumes.push(data.volume);
    }
    
    // æ•°æ®å¹³æ»‘å¤„ç†ï¼šç§»é™¤å¼‚å¸¸å€¼
    let prices = smooth_price_data(&prices);
    let volumes = smooth_volume_data(&volumes);
    
    // ä¸ºæ¯å¤©å‡†å¤‡ä¸€ä¸ªç‰¹å¾å‘é‡ï¼ˆåŠ¨æ€è®¡ç®—æ‰€éœ€çš„å†å²çª—å£ï¼‰
    let required_days = request.features.iter()
        .map(|f| get_feature_required_days(f))
        .max()
        .unwrap_or(20);
    
    // ä½¿ç”¨è¾ƒå°çš„lookback_windowï¼Œä½†ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è®¡ç®—æ‰€æœ‰ç‰¹å¾
    let lookback_window = required_days.max(30).min(prices.len() / 2);
    
    println!("ğŸ“Š ç‰¹å¾è®¡ç®—çª—å£: {}å¤©, æ€»ä»·æ ¼æ•°æ®: {}å¤©", lookback_window, prices.len());
    
    for i in lookback_window..prices.len() {
        let mut feature_vector = Vec::new();
        
        // æå–è¯·æ±‚ä¸­æŒ‡å®šçš„ç‰¹å¾
        for feature_name in &request.features {
            let feature_value = calculate_feature_value(
                feature_name, 
                &prices, 
                &volumes, 
                i, 
                lookback_window
            )?;
            feature_vector.push(feature_value);
        }
        
        features_matrix.push(feature_vector);
    }
    
    println!("ğŸ”¢ ç”Ÿæˆç‰¹å¾çŸ©é˜µ: {}è¡Œ x {}åˆ—", features_matrix.len(), 
             if features_matrix.is_empty() { 0 } else { features_matrix[0].len() });
    
    // ç§»é™¤å‰é¢çš„æ•°æ®ï¼Œå› ä¸ºæ²¡æœ‰è®¡ç®—ç‰¹å¾
    dates = dates[lookback_window..].to_vec();
    let valid_prices = prices[lookback_window..].to_vec();
    
    // åˆ›å»ºç›®æ ‡å˜é‡: ä½¿ç”¨æœªæ¥nå¤©çš„ä»·æ ¼å˜åŒ–ç‡
    let pred_days = request.prediction_days;
    let mut targets = Vec::new();
    
    // é˜²æ­¢æ•´æ•°æº¢å‡ºï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œé¢„æµ‹
    if features_matrix.len() <= pred_days {
        return Err(candle_core::Error::Msg(format!(
            "æ•°æ®ä¸è¶³ä»¥è¿›è¡Œ{}å¤©é¢„æµ‹ï¼Œå½“å‰ç‰¹å¾æ•°æ®{}å¤©ï¼Œéœ€è¦è‡³å°‘{}å¤©", 
            pred_days, features_matrix.len(), pred_days + 1
        )));
    }
    
    for i in 0..(features_matrix.len() - pred_days) {
        let current_price = valid_prices[i];
        
        // è®¡ç®—æœªæ¥å‡ å¤©çš„å¹³å‡ä»·æ ¼ï¼Œå‡å°‘å™ªéŸ³
        let future_prices: Vec<f64> = (1..=pred_days)
            .map(|day| valid_prices.get(i + day).copied().unwrap_or(current_price))
            .collect();
        
        let future_avg_price = future_prices.iter().sum::<f64>() / future_prices.len() as f64;
        let change_rate = (future_avg_price - current_price) / current_price;
        
        // é™åˆ¶å˜åŒ–ç‡èŒƒå›´ï¼Œé¿å…æç«¯å€¼å½±å“è®­ç»ƒ
        let clamped_change_rate = change_rate.clamp(-0.5, 0.5);
        targets.push(clamped_change_rate);
    }
    
    // æˆªæ–­ç‰¹å¾çŸ©é˜µï¼Œä½¿å…¶ä¸ç›®æ ‡å˜é‡é•¿åº¦åŒ¹é…
    features_matrix.truncate(targets.len());
    dates.truncate(targets.len());
    
    // ç‰¹å¾æ ‡å‡†åŒ–
    let (normalized_features, feature_stats) = normalize_features(&features_matrix)?;
    
    // åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† - ä½¿ç”¨æ—¶é—´åºåˆ—åˆ’åˆ†
    let train_size = (targets.len() as f64 * request.train_test_split) as usize;
    
    // è½¬æ¢ä¸ºå¼ é‡
    let features_len = normalized_features[0].len();
    let x_train_vec: Vec<f64> = normalized_features[0..train_size].iter()
        .flat_map(|v| v.iter().copied())
        .collect();
    
    let y_train_vec: Vec<f64> = targets[0..train_size].to_vec();
    
    let x_test_vec: Vec<f64> = normalized_features[train_size..].iter()
        .flat_map(|v| v.iter().copied())
        .collect();
    
    let y_test_vec: Vec<f64> = targets[train_size..].to_vec();
    
    // åˆ›å»ºå¼ é‡ï¼Œä½¿ç”¨F32ç±»å‹ä»¥åŒ¹é…æ¨¡å‹æƒé‡
    let x_train_f32: Vec<f32> = x_train_vec.iter().map(|&x| x as f32).collect();
    let y_train_f32: Vec<f32> = y_train_vec.iter().map(|&y| y as f32).collect();
    let x_test_f32: Vec<f32> = x_test_vec.iter().map(|&x| x as f32).collect();
    let y_test_f32: Vec<f32> = y_test_vec.iter().map(|&y| y as f32).collect();
    
    let x_train = Tensor::from_slice(&x_train_f32, &[train_size, features_len], &device)?;
    let y_train = Tensor::from_slice(&y_train_f32, &[train_size, 1], &device)?;
    
    let test_size = targets.len() - train_size;
    let x_test = Tensor::from_slice(&x_test_f32, &[test_size, features_len], &device)?;
    let y_test = Tensor::from_slice(&y_test_f32, &[test_size, 1], &device)?;
    
    println!("æ•°æ®é¢„å¤„ç†å®Œæˆ: è®­ç»ƒé›†{}æ ·æœ¬, æµ‹è¯•é›†{}æ ·æœ¬, ç‰¹å¾ç»´åº¦{}", 
             train_size, test_size, features_len);
    
    Ok((x_train, y_train, x_test, y_test, dates))
}

// æ•°æ®å¹³æ»‘å¤„ç†å‡½æ•°
fn smooth_price_data(prices: &[f64]) -> Vec<f64> {
    let mut smoothed = prices.to_vec();
    
    // ä½¿ç”¨ä¸­ä½æ•°æ»¤æ³¢å™¨ç§»é™¤ä»·æ ¼å¼‚å¸¸å€¼
    for i in 2..smoothed.len()-2 {
        let window: Vec<f64> = smoothed[i-2..=i+2].to_vec();
        let mut sorted_window = window.clone();
        sorted_window.sort_by(|a, b| a.partial_cmp(b).unwrap());
        let median = sorted_window[2];
        
        // å¦‚æœå½“å‰å€¼ä¸ä¸­ä½æ•°ç›¸å·®è¶…è¿‡20%ï¼Œç”¨ä¸­ä½æ•°æ›¿æ¢
        if (smoothed[i] - median).abs() / median > 0.2 {
            smoothed[i] = median;
        }
    }
    
    smoothed
}

fn smooth_volume_data(volumes: &[i64]) -> Vec<i64> {
    let mut smoothed = volumes.to_vec();
    
    // ç§»é™¤æˆäº¤é‡å¼‚å¸¸å€¼
    for i in 2..smoothed.len()-2 {
        let window: Vec<i64> = smoothed[i-2..=i+2].to_vec();
        let avg = window.iter().sum::<i64>() as f64 / window.len() as f64;
        
        // å¦‚æœå½“å‰å€¼ä¸å¹³å‡å€¼ç›¸å·®è¶…è¿‡5å€ï¼Œç”¨å¹³å‡å€¼æ›¿æ¢
        if (smoothed[i] as f64 - avg).abs() / avg > 5.0 {
            smoothed[i] = avg as i64;
        }
    }
    
    smoothed
}

// è·å–ç‰¹å¾æ‰€éœ€çš„å†å²å¤©æ•°
fn get_feature_required_days(feature_name: &str) -> usize {
    match feature_name {
        "close" | "volume" | "change_percent" => 1,
        "ma5" => 5,
        "ma10" => 10,
        "ma20" | "bollinger" => 20,
        "rsi" | "stochastic_k" | "stochastic_d" => 14,
        "macd" => 26,
        "momentum" => 10,
        _ => 1,
    }
}

// è®¡ç®—å•ä¸ªç‰¹å¾å€¼
fn calculate_feature_value(
    feature_name: &str,
    prices: &[f64],
    volumes: &[i64],
    index: usize,
    _lookback_window: usize,
) -> Result<f64, candle_core::Error> {
    match feature_name {
        "close" => {
            Ok(prices[index])
        },
        "volume" => {
            Ok(volumes[index] as f64)
        },
        "change_percent" => {
            if index > 0 {
                let prev_price = prices[index - 1];
                let change = (prices[index] - prev_price) / prev_price;
                Ok(change)
            } else {
                Ok(0.0)
            }
        },
        "ma5" => {
            if index >= 4 {
                let ma5 = prices[index-4..=index].iter().sum::<f64>() / 5.0;
                Ok(ma5)
            } else {
                Ok(prices[index])
            }
        },
        "ma10" => {
            if index >= 9 {
                let ma10 = prices[index-9..=index].iter().sum::<f64>() / 10.0;
                Ok(ma10)
            } else {
                Ok(prices[index])
            }
        },
        "ma20" => {
            if index >= 19 {
                let ma20 = prices[index-19..=index].iter().sum::<f64>() / 20.0;
                Ok(ma20)
            } else {
                Ok(prices[index])
            }
        },
        "rsi" => {
            if index >= 14 {
                Ok(calculate_rsi(&prices[index-14..=index]))
            } else { 
                Ok(50.0) // é»˜è®¤ä¸­æ€§RSI
            }
        },
        "macd" => {
            if index >= 25 {
                Ok(calculate_macd(&prices[index-25..=index]))
            } else {
                Ok(0.0)
            }
        },
        "bollinger" => {
            if index >= 19 {
                Ok(calculate_bollinger_position(&prices[index-19..=index], prices[index]))
            } else {
                Ok(0.0)
            }
        },
        "stochastic_k" => {
            if index >= 13 {
                Ok(calculate_stochastic_k(&prices[index-13..=index], prices[index]))
            } else {
                Ok(0.5)
            }
        },
        "stochastic_d" => {
            if index >= 15 {
                // è®¡ç®—å‰3å¤©çš„Kå€¼çš„å¹³å‡å€¼
                let k_values: Vec<f64> = (0..3)
                    .map(|i| {
                        let k_index = index - i;
                        if k_index >= 13 {
                            calculate_stochastic_k(&prices[k_index-13..=k_index], prices[k_index])
                        } else {
                            0.5
                        }
                    })
                    .collect();
                Ok(k_values.iter().sum::<f64>() / k_values.len() as f64)
            } else {
                Ok(0.5)
            }
        },
        "momentum" => {
            if index >= 10 {
                let momentum = prices[index] / prices[index-10] - 1.0;
                Ok(momentum)
            } else {
                Ok(0.0)
            }
        },
        _ => {
            Ok(0.0)
        }
    }
}

// ç‰¹å¾æ ‡å‡†åŒ–
fn normalize_features(features: &[Vec<f64>]) -> Result<(Vec<Vec<f64>>, Vec<(f64, f64)>), candle_core::Error> {
    if features.is_empty() {
        return Ok((Vec::new(), Vec::new()));
    }
    
    let feature_count = features[0].len();
    let mut stats = Vec::with_capacity(feature_count);
    let mut normalized = vec![vec![0.0; feature_count]; features.len()];
    
    // è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®
    for feature_idx in 0..feature_count {
        let values: Vec<f64> = features.iter().map(|row| row[feature_idx]).collect();
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        let variance = values.iter()
            .map(|v| (v - mean).powi(2))
            .sum::<f64>() / values.len() as f64;
        let std_dev = variance.sqrt().max(1e-8); // é¿å…é™¤é›¶
        
        stats.push((mean, std_dev));
        
        // æ ‡å‡†åŒ–è¯¥ç‰¹å¾
        for (row_idx, row) in features.iter().enumerate() {
            normalized[row_idx][feature_idx] = (row[feature_idx] - mean) / std_dev;
        }
    }
    
    Ok((normalized, stats))
}

// RSIè®¡ç®—å‡½æ•°
fn calculate_rsi(prices: &[f64]) -> f64 {
    if prices.len() < 2 {
        return 50.0;
    }
    
    let mut gains = 0.0;
    let mut losses = 0.0;
    
    for i in 1..prices.len() {
        let change = prices[i] - prices[i-1];
        if change > 0.0 {
            gains += change;
        } else {
            losses += -change;
        }
    }
    
    gains /= (prices.len() - 1) as f64;
    losses /= (prices.len() - 1) as f64;
    
    if losses == 0.0 {
        100.0
    } else {
        100.0 - (100.0 / (1.0 + (gains / losses)))
    }
}

// MACDè®¡ç®—å‡½æ•°
fn calculate_macd(prices: &[f64]) -> f64 {
    if prices.len() < 26 {
        return 0.0;
    }
    
    let ema12 = calculate_ema(prices, 12);
    let ema26 = calculate_ema(prices, 26);
    ema12 - ema26
}

// å¸ƒæ—å¸¦ä½ç½®è®¡ç®—
fn calculate_bollinger_position(prices: &[f64], current_price: f64) -> f64 {
    if prices.len() < 20 {
        return 0.0;
    }
    
    let ma = prices.iter().sum::<f64>() / prices.len() as f64;
    let variance = prices.iter()
        .map(|p| (p - ma).powi(2))
        .sum::<f64>() / prices.len() as f64;
    let std_dev = variance.sqrt();
    
    let upper_band = ma + 2.0 * std_dev;
    let lower_band = ma - 2.0 * std_dev;
    
    if upper_band == lower_band {
        0.0
    } else {
        (current_price - lower_band) / (upper_band - lower_band) - 0.5
    }
}

// éšæœºæŒ‡æ ‡Kå€¼è®¡ç®—
fn calculate_stochastic_k(prices: &[f64], current_price: f64) -> f64 {
    if prices.is_empty() {
        return 0.5;
    }
    
    let highest = prices.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
    let lowest = prices.iter().fold(f64::INFINITY, |a, &b| a.min(b));
    
    if highest == lowest {
        0.5
    } else {
        (current_price - lowest) / (highest - lowest)
    }
}

// ä»æ•°æ®åº“è·å–å†å²æ•°æ®
async fn get_historical_data_from_db(symbol: &str, start_date: &str, end_date: &str) -> Result<Vec<HistoricalDataType>, String> {
    // åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„æ•°æ®åº“è¿æ¥
    use sqlx::{sqlite::SqlitePoolOptions, Pool, Sqlite};
    
    // ä½¿ç”¨åŠ¨æ€æ•°æ®åº“è·¯å¾„æŸ¥æ‰¾
    let db_path = find_database_path()
        .ok_or_else(|| "æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶".to_string())?;
    
    let connection_string = format!("sqlite://{}", db_path.display());
    let pool = SqlitePoolOptions::new()
        .max_connections(1)
        .connect(&connection_string)
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    let records = sqlx::query_as::<_, HistoricalDataType>(
        r#"SELECT * FROM historical_data 
           WHERE symbol = ? AND date BETWEEN ? AND ?
           ORDER BY date ASC"#
    )
    .bind(symbol)
    .bind(start_date)
    .bind(end_date)
    .fetch_all(&pool)
    .await
    .map_err(|e| format!("æŸ¥è¯¢å†å²æ•°æ®å¤±è´¥: {}", e))?;
    
    Ok(records)
}

// å†å²æ•°æ®ç»“æ„ä½“
#[derive(Debug, Clone)]
struct HistoricalDataType {
    pub symbol: String,
    pub date: String,
    pub open: f64,
    pub close: f64,
    pub high: f64,
    pub low: f64,
    pub volume: i64,
    pub amount: f64,
    pub amplitude: f64,
    pub turnover_rate: f64,
    pub change: f64,
    pub change_percent: f64,
}

// å®ç°FromRowç‰¹å¾ï¼Œä½¿å…¶å¯ä»¥ä»æ•°æ®åº“è¡Œè½¬æ¢
impl<'r> sqlx::FromRow<'r, sqlx::sqlite::SqliteRow> for HistoricalDataType {
    fn from_row(row: &'r sqlx::sqlite::SqliteRow) -> Result<Self, sqlx::Error> {
        Ok(Self {
            symbol: row.try_get("symbol")?,
            date: row.try_get("date")?,
            open: row.try_get("open")?,
            close: row.try_get("close")?,
            high: row.try_get("high")?,
            low: row.try_get("low")?,
            volume: row.try_get("volume")?,
            amount: row.try_get("amount")?,
            amplitude: row.try_get("amplitude")?,
            turnover_rate: row.try_get("turnover_rate")?,
            change: row.try_get("change")?,
            change_percent: row.try_get("change_percent")?,
        })
    }
}

// è®¡ç®—æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿
fn calculate_ema(data: &[f64], period: usize) -> f64 {
    if data.is_empty() || period == 0 || data.len() < period {
        return 0.0;
    }
    
    let multiplier = 2.0 / (period as f64 + 1.0);
    let mut ema = data[0];
    
    for i in 1..data.len() {
        ema = (data[i] - ema) * multiplier + ema;
    }
    
    ema
}

// è®­ç»ƒæ¨¡å‹å‡½æ•°
pub async fn train_candle_model(request: TrainingRequest) -> std::result::Result<TrainingResult, String> {
    let model_id = Uuid::new_v4().to_string();
    let model_type = request.model_type.clone();
    
    // å‡†å¤‡æ•°æ®
    let (x_train, y_train, x_test, y_test, _) = prepare_stock_data(&request).await
        .map_err(|e| format!("æ•°æ®å‡†å¤‡å¤±è´¥: {}", e))?;
    
    // è®¾ç½®è®¾å¤‡
    let device = Device::Cpu;
    
    // åˆ›å»ºæ¨¡å‹é…ç½®
    let config = ModelConfig {
        model_type: model_type.clone(),
        input_size: request.features.len(),
        hidden_size: 64, // éšè—å±‚å¤§å°
        output_size: 1,  // è¾“å‡ºå°ºå¯¸ (è‚¡ä»·)
        dropout: request.dropout,
        learning_rate: request.learning_rate,
        n_layers: 2,     // é»˜è®¤å€¼
        n_heads: 4,      // é»˜è®¤å€¼
        max_seq_len: 60, // é»˜è®¤å€¼
    };
    
    // åˆ›å»ºæ¨¡å‹
    let (varmap, model) = create_model(&config, &device)
        .map_err(|e| format!("æ¨¡å‹åˆ›å»ºå¤±è´¥: {}", e))?;
    
    // åˆ›å»ºä¼˜åŒ–å™¨
    let mut optimizer = AdamW::new_lr(varmap.all_vars(), request.learning_rate)
        .map_err(|e| format!("ä¼˜åŒ–å™¨åˆ›å»ºå¤±è´¥: {}", e))?;
    
    // è®­ç»ƒæ¨¡å‹
    let batch_size = request.batch_size;
    let num_batches = x_train.dim(0).unwrap() / batch_size;
    
    for epoch in 0..request.epochs {
        let mut epoch_loss = 0.0;
        
        for batch_idx in 0..num_batches {
            let batch_start = batch_idx * batch_size;
            let x_batch = x_train.narrow(0, batch_start, batch_size)
                .map_err(|e| format!("æ‰¹æ¬¡æ•°æ®å‡†å¤‡å¤±è´¥: {}", e))?;
            let y_batch = y_train.narrow(0, batch_start, batch_size)
                .map_err(|e| format!("æ‰¹æ¬¡æ•°æ®å‡†å¤‡å¤±è´¥: {}", e))?;
            
            // å‰å‘ä¼ æ’­
            let output = model.forward(&x_batch)
                .map_err(|e| format!("å‰å‘ä¼ æ’­å¤±è´¥: {}", e))?;
            
            // è®¡ç®—æŸå¤± (å‡æ–¹è¯¯å·®)
            let loss = output.sub(&y_batch).map_err(|e| format!("è®¡ç®—æŸå¤±å¤±è´¥: {}", e))?;
            let loss_squared = loss.sqr().map_err(|e| format!("è®¡ç®—å¹³æ–¹å¤±è´¥: {}", e))?;
            let loss = loss_squared.mean_all().map_err(|e| format!("è®¡ç®—å‡å€¼å¤±è´¥: {}", e))?;
            
            // åå‘ä¼ æ’­
            optimizer.backward_step(&loss)
                .map_err(|e| format!("åå‘ä¼ æ’­å¤±è´¥: {}", e))?;
            
            epoch_loss += loss.to_scalar::<f32>().unwrap() as f64;
        }
        
        // æ¯10ä¸ªepochè®°å½•ä¸€æ¬¡æŸå¤±
        if (epoch + 1) % 10 == 0 || epoch == 0 || epoch == request.epochs - 1 {
            println!("Epoch {}/{}, Loss: {:.4}", epoch + 1, request.epochs, epoch_loss / num_batches as f64);
        }
    }
    
    // è¯„ä¼°æ¨¡å‹
    let y_pred = model.forward(&x_test)
        .map_err(|e| format!("é¢„æµ‹å¤±è´¥: {}", e))?;
    
    // è½¬æ¢ä¸ºVecç”¨äºå‡†ç¡®ç‡è®¡ç®— - å¤„ç†ä¸åŒç»´åº¦çš„å¼ é‡
    let predictions_vec = match y_pred.dims() {
        // å¦‚æœæ˜¯1ç»´å¼ é‡ [n]
        [_] => {
            y_pred.to_vec1::<f32>().map_err(|e| format!("è½¬æ¢1ç»´é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?
                .into_iter().map(|x| x as f64).collect::<Vec<f64>>()
        },
        // å¦‚æœæ˜¯2ç»´å¼ é‡ [n, 1] 
        [_, 1] => {
            y_pred.to_vec2::<f32>().map_err(|e| format!("è½¬æ¢2ç»´é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?
                .into_iter().map(|row| row[0] as f64).collect::<Vec<f64>>()
        },
        // å¦‚æœæ˜¯å…¶ä»–2ç»´å¼ é‡ [n, m]
        [_, _] => {
            let vec2d = y_pred.to_vec2::<f32>().map_err(|e| format!("è½¬æ¢2ç»´é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?;
            vec2d.into_iter().map(|row| row[0] as f64).collect::<Vec<f64>>() // å–ç¬¬ä¸€åˆ—
        },
        // å…¶ä»–ç»´åº¦
        _ => {
            return Err("é¢„æµ‹è¾“å‡ºå¼ é‡ç»´åº¦ä¸æ”¯æŒï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®".to_string());
        }
    };
    
    let actuals_vec = match y_test.dims() {
        // å¦‚æœæ˜¯1ç»´å¼ é‡ [n]
        [_] => {
            y_test.to_vec1::<f32>().map_err(|e| format!("è½¬æ¢1ç»´å®é™…ç»“æœå¤±è´¥: {}", e))?
                .into_iter().map(|x| x as f64).collect::<Vec<f64>>()
        },
        // å¦‚æœæ˜¯2ç»´å¼ é‡ [n, 1]
        [_, 1] => {
            y_test.to_vec2::<f32>().map_err(|e| format!("è½¬æ¢2ç»´å®é™…ç»“æœå¤±è´¥: {}", e))?
                .into_iter().map(|row| row[0] as f64).collect::<Vec<f64>>()
        },
        // å¦‚æœæ˜¯å…¶ä»–2ç»´å¼ é‡ [n, m]
        [_, _] => {
            let vec2d = y_test.to_vec2::<f32>().map_err(|e| format!("è½¬æ¢2ç»´å®é™…ç»“æœå¤±è´¥: {}", e))?;
            vec2d.into_iter().map(|row| row[0] as f64).collect::<Vec<f64>>() // å–ç¬¬ä¸€åˆ—
        },
        // å…¶ä»–ç»´åº¦
        _ => {
            return Err("å®é™…å€¼å¼ é‡ç»´åº¦ä¸æ”¯æŒï¼Œè¯·æ£€æŸ¥æ•°æ®å‡†å¤‡".to_string());
        }
    };
    
    // ä½¿ç”¨çœŸå®çš„å‡†ç¡®ç‡è®¡ç®—æ–¹æ³•
    let accuracy = calculate_realistic_accuracy(&predictions_vec, &actuals_vec);
    
    // è®¡ç®—MSEå’ŒRMSEç”¨äºæ—¥å¿—æ˜¾ç¤º
    let diff = y_pred.sub(&y_test).map_err(|e| format!("è®¡ç®—MSEå¤±è´¥: {}", e))?;
    let squared_diff = diff.sqr().map_err(|e| format!("è®¡ç®—å¹³æ–¹å¤±è´¥: {}", e))?;
    let mse = squared_diff.mean_all().map_err(|e| format!("è®¡ç®—å‡å€¼å¤±è´¥: {}", e))?;
    let mse = mse.to_scalar::<f32>().unwrap() as f64;
    let rmse = mse.sqrt();
    
    println!("è¯„ä¼°ç»“æœ: MSE = {:.4}, RMSE = {:.4}, Accuracy = {:.4}% (æ–¹å‘+ä»·æ ¼ç»¼åˆ)", 
             mse, rmse, accuracy * 100.0);
    println!("ğŸ“Š é¢„æµ‹å¼ é‡ç»´åº¦: {:?}, å®é™…å¼ é‡ç»´åº¦: {:?}", y_pred.dims(), y_test.dims());
    
    // ä¿å­˜æ¨¡å‹
    let model_dir = get_model_dir(&model_id);
    fs::create_dir_all(&model_dir).map_err(|e| format!("åˆ›å»ºæ¨¡å‹ç›®å½•å¤±è´¥: {}", e))?;
    
    let model_path = model_dir.join("model.safetensors");
    save_model(&varmap, &model_path).map_err(|e| format!("æ¨¡å‹ä¿å­˜å¤±è´¥: {}", e))?;
    
    // ä¿å­˜æ¨¡å‹å…ƒæ•°æ®
    let now = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
    
    let metadata = ModelInfo {
        id: model_id,
        name: request.model_name,
        stock_code: request.stock_code,
        created_at: now,
        model_type,
        features: request.features,
        target: request.target,
        prediction_days: request.prediction_days,
        accuracy,
    };
    
    save_model_metadata(&metadata).map_err(|e| format!("å…ƒæ•°æ®ä¿å­˜å¤±è´¥: {}", e))?;
    
    Ok(TrainingResult {
        metadata,
        accuracy,
    })
}

// è‚¡ç¥¨é¢„æµ‹å‡½æ•°
pub async fn predict_with_candle(request: PredictionRequest) -> std::result::Result<Vec<Prediction>, String> {
    let model_list = list_models(&request.stock_code);
    
    if model_list.is_empty() {
        return Err("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹".to_string());
    }
    
    // è·å–æ¨¡å‹å…ƒæ•°æ®
    let metadata = if let Some(model_name) = &request.model_name {
        model_list.iter()
            .find(|m| m.name == *model_name)
            .ok_or_else(|| format!("æ‰¾ä¸åˆ°åä¸º {} çš„æ¨¡å‹", model_name))?
            .clone()
    } else {
        // å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹åç§°ï¼Œä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
        model_list[0].clone()
    };
    
    // åŠ è½½æ¨¡å‹
    let device = Device::Cpu;
    
    let config = ModelConfig {
        model_type: metadata.model_type.clone(),
        input_size: metadata.features.len(),
        hidden_size: 64,
        output_size: 1,
        dropout: 0.0, // é¢„æµ‹æ—¶ä¸ä½¿ç”¨dropout
        learning_rate: 0.001,
        n_layers: 2,   // é»˜è®¤å€¼
        n_heads: 4,    // é»˜è®¤å€¼
        max_seq_len: 60, // é»˜è®¤å€¼
    };
    
    let mut varmap = VarMap::new();
    
    let (_, model) = create_model(&config, &device)
        .map_err(|e| format!("æ¨¡å‹åˆ›å»ºå¤±è´¥: {}", e))?;
    
    let model_path = get_model_dir(&metadata.id).join("model.safetensors");
    varmap.load(&model_path).map_err(|e| format!("æ¨¡å‹åŠ è½½å¤±è´¥: {}", e))?;
    
    // è·å–æœ€è¿‘çš„çœŸå®å¸‚åœºæ•°æ®ç”¨äºé¢„æµ‹
    let (current_price, dates, prices, volumes) = get_recent_market_data(&request.stock_code, 60).await
        .map_err(|e| format!("è·å–å¸‚åœºæ•°æ®å¤±è´¥: {}", e))?;
    
    if prices.len() < 20 {
        return Err("å†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹ï¼Œéœ€è¦è‡³å°‘20å¤©æ•°æ®".to_string());
    }
    
    // è®¡ç®—ç‰¹å¾å‘é‡
    let mut features = Vec::new();
    let last_idx = prices.len() - 1;
    
    // ä¸ºæ¯ä¸ªç‰¹å¾è®¡ç®—å€¼
    for feature_name in &metadata.features {
        match feature_name.as_str() {
            "close" => {
                // å½’ä¸€åŒ–æ”¶ç›˜ä»·
                let price_min = prices.iter().fold(f64::INFINITY, |a, &b| a.min(b));
                let price_max = prices.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
                let price_range = price_max - price_min;
                let normalized = if price_range > 0.0 {
                    (current_price - price_min) / price_range
                } else {
                    0.5 // å¦‚æœä»·æ ¼æ²¡æœ‰å˜åŒ–ï¼Œä½¿ç”¨ä¸­é—´å€¼
                };
                features.push(normalized);
            },
            "volume" => {
                // å½’ä¸€åŒ–æˆäº¤é‡
                let latest_volume = volumes[last_idx];
                let vol_min = volumes.iter().fold(i64::MAX, |a, &b| a.min(b));
                let vol_max = volumes.iter().fold(i64::MIN, |a, &b| a.max(b));
                let vol_range = (vol_max - vol_min) as f64;
                let normalized = if vol_range > 0.0 {
                    (latest_volume - vol_min) as f64 / vol_range
                } else {
                    0.5 // å¦‚æœæˆäº¤é‡æ²¡æœ‰å˜åŒ–ï¼Œä½¿ç”¨ä¸­é—´å€¼
                };
                features.push(normalized);
            },
            "change_percent" => {
                // è®¡ç®—ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”
                let prev_price = prices[last_idx - 1];
                let change = (current_price - prev_price) / prev_price;
                let normalized = (change / 0.1).clamp(-1.0, 1.0); // å‡è®¾æ­£å¸¸å˜åŒ–ç‡åœ¨Â±10%å†…
                features.push(normalized);
            },
            "ma5" => {
                // 5æ—¥ç§»åŠ¨å¹³å‡çº¿
                if prices.len() >= 5 {
                    let ma5 = prices[prices.len()-5..].iter().sum::<f64>() / 5.0;
                    let normalized = (ma5 - current_price) / current_price;
                    features.push(normalized);
                } else {
                    features.push(0.0);
                }
            },
            "ma10" => {
                // 10æ—¥ç§»åŠ¨å¹³å‡çº¿
                if prices.len() >= 10 {
                    let ma10 = prices[prices.len()-10..].iter().sum::<f64>() / 10.0;
                    let normalized = (ma10 - current_price) / current_price;
                    features.push(normalized);
                } else {
                    features.push(0.0);
                }
            },
            "ma20" => {
                // 20æ—¥ç§»åŠ¨å¹³å‡çº¿
                if prices.len() >= 20 {
                    let ma20 = prices[prices.len()-20..].iter().sum::<f64>() / 20.0;
                    let normalized = (ma20 - current_price) / current_price;
                    features.push(normalized);
                } else {
                    features.push(0.0);
                }
            },
            "rsi" => {
                // RSIè®¡ç®—
                if prices.len() >= 15 {
                    let gains = prices[prices.len()-15..prices.len()-1]
                        .iter()
                        .zip(prices[prices.len()-14..].iter())
                        .map(|(prev, curr)| {
                            let diff = curr - prev;
                            if diff > 0.0 { diff } else { 0.0 }
                        })
                        .sum::<f64>() / 14.0;
                        
                    let losses = prices[prices.len()-15..prices.len()-1]
                        .iter()
                        .zip(prices[prices.len()-14..].iter())
                        .map(|(prev, curr)| {
                            let diff = prev - curr;
                            if diff > 0.0 { diff } else { 0.0 }
                        })
                        .sum::<f64>() / 14.0;
                        
                    let rsi = if losses == 0.0 { 
                        100.0 
                    } else { 
                        100.0 - (100.0 / (1.0 + (gains / losses))) 
                    };
                    
                    features.push(rsi / 100.0);
                } else {
                    features.push(0.5); // é»˜è®¤ä¸­æ€§RSI
                }
            },
            "macd" => {
                // MACDè®¡ç®—
                if prices.len() >= 26 {
                    let ema12 = prices[prices.len()-26..].iter().sum::<f64>() / 12.0;
                    let ema26 = prices[prices.len()-26..].iter().sum::<f64>() / 26.0;
                    let macd = ema12 - ema26;
                    let normalized = macd / current_price;
                    features.push(normalized);
                } else {
                    features.push(0.0);
                }
            },
            "bollinger" => {
                // å¸ƒæ—å¸¦è®¡ç®—
                if prices.len() >= 20 {
                    let ma20 = prices[prices.len()-20..].iter().sum::<f64>() / 20.0;
                    let variance = prices[prices.len()-20..]
                        .iter()
                        .map(|p| (p - ma20).powi(2))
                        .sum::<f64>() / 20.0;
                    let std_dev = variance.sqrt();
                    let upper_band = ma20 + 2.0 * std_dev;
                    let lower_band = ma20 - 2.0 * std_dev;
                    
                    // è®¡ç®—ä»·æ ¼åœ¨å¸ƒæ—å¸¦ä¸­çš„ç›¸å¯¹ä½ç½® (-1åˆ°1)
                    let position = if upper_band > lower_band {
                        2.0 * (current_price - lower_band) / (upper_band - lower_band) - 1.0
                    } else {
                        0.0
                    };
                    
                    features.push(position);
                } else {
                    features.push(0.0);
                }
            },
            "stochastic_k" => {
                // Kå€¼è®¡ç®—
                if prices.len() >= 14 {
                    let highest_high = prices[prices.len()-14..].iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
                    let lowest_low = prices[prices.len()-14..].iter().fold(f64::INFINITY, |a, &b| a.min(b));
                    
                    let k = if highest_high > lowest_low {
                        (current_price - lowest_low) / (highest_high - lowest_low)
                    } else {
                        0.5
                    };
                    
                    features.push(k);
                } else {
                    features.push(0.5);
                }
            },
            "stochastic_d" => {
                // Då€¼è®¡ç®—(Kå€¼çš„3æ—¥ç§»åŠ¨å¹³å‡)
                if prices.len() >= 14 {
                    let highest_high = prices[prices.len()-14..].iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
                    let lowest_low = prices[prices.len()-14..].iter().fold(f64::INFINITY, |a, &b| a.min(b));
                    
                    let k1 = if highest_high > lowest_low {
                        (current_price - lowest_low) / (highest_high - lowest_low)
                    } else {
                        0.5
                    };
                    
                    let k2 = if highest_high > lowest_low && prices.len() > 1 {
                        (prices[prices.len()-2] - lowest_low) / (highest_high - lowest_low)
                    } else {
                        0.5
                    };
                    
                    let k3 = if highest_high > lowest_low && prices.len() > 2 {
                        (prices[prices.len()-3] - lowest_low) / (highest_high - lowest_low)
                    } else {
                        0.5
                    };
                    
                    let d = (k1 + k2 + k3) / 3.0;
                    features.push(d);
                } else {
                    features.push(0.5);
                }
            },
            "momentum" => {
                // åŠ¨é‡æŒ‡æ ‡(å½“å‰ä»·æ ¼ä¸næ—¥å‰ä»·æ ¼çš„æ¯”ç‡)
                let n = 10; // ä½¿ç”¨10æ—¥åŠ¨é‡
                if prices.len() > n {
                    let price_n_days_ago = prices[prices.len()-n-1];
                    let momentum = current_price / price_n_days_ago - 1.0;
                    let normalized = (momentum / 0.2).clamp(-1.0, 1.0); // å‡è®¾æ­£å¸¸åŠ¨é‡åœ¨Â±20%å†…
                    features.push(normalized);
                } else {
                    features.push(0.0);
                }
            },
            _ => {
                // æœªçŸ¥ç‰¹å¾ï¼Œæ·»åŠ 0å€¼
                features.push(0.0);
            }
        }
    }
    
    // åˆ›å»ºè¾“å…¥å¼ é‡ï¼Œè½¬æ¢ä¸ºF32ç±»å‹ä»¥åŒ¹é…æ¨¡å‹æƒé‡
    let features_f32: Vec<f32> = features.iter().map(|&x| x as f32).collect();
    let input_tensor = Tensor::from_slice(&features_f32, &[1, features.len()], &device)
        .map_err(|e| format!("åˆ›å»ºè¾“å…¥å¼ é‡å¤±è´¥: {}", e))?;
    
    // è¿›è¡Œé¢„æµ‹
    let output = model.forward(&input_tensor)
        .map_err(|e| format!("é¢„æµ‹å¤±è´¥: {}", e))?;
    
    // è·å–é¢„æµ‹ç»“æœ(ä»·æ ¼å˜åŒ–ç‡)
    // æ›´å®‰å…¨åœ°å¤„ç†è¾“å‡ºå¼ é‡ï¼Œæ”¯æŒä¸åŒç»´åº¦
    let raw_change_rate = match output.dims() {
        // å¦‚æœæ˜¯æ ‡é‡ []
        [] => {
            output.to_scalar::<f32>().map_err(|e| format!("è·å–æ ‡é‡é¢„æµ‹ç»“æœå¤±è´¥: {}", e))? as f64
        },
        // å¦‚æœæ˜¯ [1] ç»´åº¦
        [1] => {
            let output_vec = output.to_vec1::<f32>().map_err(|e| format!("è·å–1ç»´é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?;
            output_vec[0] as f64
        },
        // å¦‚æœæ˜¯ [1, 1] ç»´åº¦
        [1, 1] => {
            let output_vec = output.to_vec2::<f32>().map_err(|e| format!("è·å–2ç»´é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?;
            output_vec[0][0] as f64
        },
        // å¦‚æœæ˜¯å…¶ä»–ç»´åº¦ï¼Œå°è¯•è·å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        _ => {
            // å±•å¹³ä¸º1ç»´æ•°ç»„å¹¶è·å–ç¬¬ä¸€ä¸ªå€¼
            let output_vec = output.flatten_all().map_err(|e| format!("å±•å¹³å¼ é‡å¤±è´¥: {}", e))?
                .to_vec1::<f32>().map_err(|e| format!("è½¬æ¢ä¸ºå‘é‡å¤±è´¥: {}", e))?;
            if output_vec.is_empty() {
                return Err("é¢„æµ‹è¾“å‡ºä¸ºç©º".to_string());
            }
            output_vec[0] as f64
        }
    };
    
    // åŸºäºå†å²æ•°æ®è®¡ç®—çœŸå®çš„ä»·æ ¼å˜åŒ–æ¨¡å¼
    let historical_volatility = calculate_historical_volatility(&prices);
    let recent_trend = calculate_recent_trend(&prices);
    let support_resistance = calculate_support_resistance(&prices, current_price);
    
    println!("ğŸ“Š å†å²æ³¢åŠ¨ç‡: {:.4}, è¿‘æœŸè¶‹åŠ¿: {:.4}, æ”¯æ’‘é˜»åŠ›: {:.4}", 
             historical_volatility, recent_trend, support_resistance);
    
    // ç”Ÿæˆé¢„æµ‹
    let mut predictions = Vec::new();
    let mut last_price = current_price;
    
    // è·å–æœ€åä¸€ä¸ªæ—¥æœŸï¼Œç”¨äºè®¡ç®—é¢„æµ‹æ—¥æœŸ
    let last_date = chrono::NaiveDate::parse_from_str(&dates.last().unwrap_or(&"2023-01-01".to_string()), "%Y-%m-%d")
        .unwrap_or_else(|_| chrono::Local::now().naive_local().date());
    
    for day in 1..=request.prediction_days {
        // åˆ›å»ºç›®æ ‡æ—¥æœŸ - ä½¿ç”¨Aè‚¡äº¤æ˜“æ—¥è§„åˆ™
        let mut target_date = last_date;
        // å‘å‰æ¨è¿›æŒ‡å®šçš„äº¤æ˜“æ—¥æ•°
        for _ in 0..day {
            target_date = get_next_trading_day(target_date);
        }
        let date_str = target_date.format("%Y-%m-%d").to_string();
        
        // ç»¼åˆå¤šä¸ªå› ç´ è®¡ç®—é¢„æµ‹å˜åŒ–ç‡
        let mut predicted_change_rate = 0.0;
        
        // 1. æ¨¡å‹é¢„æµ‹æƒé‡ (40%)
        let model_weight = 0.4;
        let normalized_model_output = (raw_change_rate * historical_volatility).tanh(); // ä½¿ç”¨tanhé™åˆ¶èŒƒå›´
        predicted_change_rate += normalized_model_output * model_weight;
        
        // 2. å†å²è¶‹åŠ¿æƒé‡ (30%)
        let trend_weight = 0.3;
        let trend_decay = 0.95_f64.powi(day as i32); // è¶‹åŠ¿éšæ—¶é—´è¡°å‡
        predicted_change_rate += recent_trend * trend_weight * trend_decay;
        
        // 3. éšæœºæ³¢åŠ¨æƒé‡ (20%)
        let random_weight = 0.2;
        let random_factor = (rand::random::<f64>() - 0.5) * historical_volatility * 2.0;
        predicted_change_rate += random_factor * random_weight;
        
        // 4. æ”¯æ’‘é˜»åŠ›å½±å“ (10%)
        let sr_weight = 0.1;
        let sr_factor = if last_price > current_price * 1.05 {
            // ä»·æ ¼è¿‡é«˜ï¼Œé˜»åŠ›ä½å½±å“
            -support_resistance * 0.5
        } else if last_price < current_price * 0.95 {
            // ä»·æ ¼è¿‡ä½ï¼Œæ”¯æ’‘ä½å½±å“
            support_resistance * 0.5
        } else {
            0.0
        };
        predicted_change_rate += sr_factor * sr_weight;
        
        // åº”ç”¨Aè‚¡æ¶¨è·Œå¹…é™åˆ¶ (Â±10%)
        let clamped_change_rate = clamp_daily_change(predicted_change_rate * 100.0) / 100.0;
        
        // è®¡ç®—é¢„æµ‹ä»·æ ¼
        let predicted_price = last_price * (1.0 + clamped_change_rate);
        
        // è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”
        let change_percent = clamped_change_rate * 100.0;
        
        // åŸºäºå¤šå› ç´ è®¡ç®—ç½®ä¿¡åº¦
        let base_confidence = metadata.accuracy * 0.8; // åŸºç¡€ç½®ä¿¡åº¦ç¨å¾®é™ä½
        let volatility_penalty = (historical_volatility * 5.0).min(0.3); // æ³¢åŠ¨æ€§æƒ©ç½š
        let trend_consistency = (recent_trend * predicted_change_rate).max(0.0); // è¶‹åŠ¿ä¸€è‡´æ€§å¥–åŠ±
        let distance_penalty = (change_percent.abs() / 10.0).min(0.4); // é¢„æµ‹å˜åŒ–è¶Šå¤§ï¼Œç½®ä¿¡åº¦è¶Šä½
        let time_decay = 0.92_f64.powi(day as i32); // æ—¶é—´è¡°å‡
        
        let confidence = (base_confidence * time_decay + trend_consistency * 0.1 - volatility_penalty - distance_penalty).clamp(0.2, 0.85);
        
        // æ·»åŠ é¢„æµ‹ç»“æœ
        predictions.push(Prediction {
            target_date: date_str,
            predicted_price,
            predicted_change_percent: change_percent,
            confidence,
        });
        
        // æ›´æ–°ä¸Šä¸€ä¸ªé¢„æµ‹ä»·æ ¼
        last_price = predicted_price;
        
        // è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        println!("ğŸ“ˆ ç¬¬{}å¤©é¢„æµ‹: ä»·æ ¼={:.2}, å˜åŒ–={:.2}%, ç½®ä¿¡åº¦={:.2}%", 
                 day, predicted_price, change_percent, confidence * 100.0);
    }
    
    Ok(predictions)
}

// ä»æ•°æ®åº“è·å–æœ€è¿‘çš„å¸‚åœºæ•°æ®
async fn get_recent_market_data(symbol: &str, days: usize) -> Result<(f64, Vec<String>, Vec<f64>, Vec<i64>), String> {
    // åˆ›å»ºä¸´æ—¶æ•°æ®åº“è¿æ¥
    use sqlx::{sqlite::SqlitePoolOptions, Pool, Sqlite};
    use chrono::Local;
    
    // è®¡ç®—å¼€å§‹æ—¥æœŸï¼ˆå¤§å¹…å¢åŠ æ•°æ®è·å–èŒƒå›´ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®è¿›è¡ŒæŠ€æœ¯åˆ†æï¼‰
    let end_date = Local::now().naive_local().date();
    let buffer_days = 60; // å¢åŠ ç¼“å†²æœŸåˆ°60å¤©ï¼Œåº”å¯¹èŠ‚å‡æ—¥
    // è‡³å°‘è·å–1å¹´çš„æ•°æ®ï¼Œæˆ–è€…ç”¨æˆ·æŒ‡å®šå¤©æ•°+ç¼“å†²æœŸï¼Œå–æ›´å¤§å€¼
    let total_days = std::cmp::max(365, days + buffer_days); 
    let start_date = end_date - chrono::Duration::days(total_days as i64);
    
    // ä½¿ç”¨åŠ¨æ€æ•°æ®åº“è·¯å¾„æŸ¥æ‰¾
    let db_path = find_database_path()
        .ok_or_else(|| "æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶".to_string())?;
    
    let connection_string = format!("sqlite://{}", db_path.display());
    let pool = SqlitePoolOptions::new()
        .max_connections(1)
        .connect(&connection_string)
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    // ä¿®æ”¹æŸ¥è¯¢ï¼Œè·å–æ›´å¤šå†å²æ•°æ®ä½†ä¿æŒåˆç†çš„é™åˆ¶
    let limit = std::cmp::max(300, days * 2); // è‡³å°‘300æ¡è®°å½•ï¼Œæˆ–è€…è¯·æ±‚å¤©æ•°çš„2å€
    let records = sqlx::query_as::<_, HistoricalDataType>(
        r#"SELECT * FROM historical_data 
           WHERE symbol = ? AND date BETWEEN ? AND ?
           ORDER BY date DESC
           LIMIT ?"#
    )
    .bind(symbol)
    .bind(start_date.format("%Y-%m-%d").to_string())
    .bind(end_date.format("%Y-%m-%d").to_string())
    .bind(limit as i32)
    .fetch_all(&pool)
    .await
    .map_err(|e| format!("æŸ¥è¯¢å†å²æ•°æ®å¤±è´¥: {}", e))?;
    
    if records.is_empty() {
        return Err(format!("æœªæ‰¾åˆ°è‚¡ç¥¨ä»£ç  {} çš„å†å²æ•°æ®", symbol));
    }
    
    // åå‘æ’åºä»¥è·å–æ—¶é—´é¡ºåºï¼ˆä»æ—§åˆ°æ–°ï¼‰
    let mut sorted_records = records;
    sorted_records.reverse();
    
    // æå–æ•°æ®
    let dates: Vec<String> = sorted_records.iter().map(|r| r.date.clone()).collect();
    let prices: Vec<f64> = sorted_records.iter().map(|r| r.close).collect();
    let volumes: Vec<i64> = sorted_records.iter().map(|r| r.volume).collect();
    
    // è·å–æœ€æ–°ä»·æ ¼
    let current_price = prices.last().copied().unwrap_or(0.0);
    
    println!("ğŸ“Š è·å–åˆ°{}æ¡å†å²æ•°æ®ç”¨äºé¢„æµ‹ï¼Œæ—¶é—´èŒƒå›´: {} åˆ° {}", 
             sorted_records.len(),
             sorted_records.first().map(|r| &r.date).unwrap_or(&"æœªçŸ¥".to_string()),
             sorted_records.last().map(|r| &r.date).unwrap_or(&"æœªçŸ¥".to_string()));
    
    Ok((current_price, dates, prices, volumes))
}

// é‡æ–°è®­ç»ƒæ¨¡å‹
pub async fn retrain_candle_model(
    model_id: String,
    epochs: u32,
    batch_size: u32,
    learning_rate: f64,
) -> std::result::Result<(), String> {
    // åŠ è½½æ¨¡å‹å…ƒæ•°æ®
    let metadata = load_model_metadata(&model_id)
        .map_err(|e| format!("åŠ è½½æ¨¡å‹å…ƒæ•°æ®å¤±è´¥: {}", e))?;
    
    // æ„å»ºè®­ç»ƒè¯·æ±‚ - ä½¿ç”¨æ›´é•¿çš„æ—¶é—´èŒƒå›´
    let end_date = chrono::Local::now().naive_local().date();
    let start_date = end_date - chrono::Duration::days(500); // ä½¿ç”¨çº¦1.5å¹´çš„æ•°æ®
    
    let request = TrainingRequest {
        stock_code: metadata.stock_code.clone(),
        model_name: metadata.name.clone(),
        start_date: start_date.format("%Y-%m-%d").to_string(),
        end_date: end_date.format("%Y-%m-%d").to_string(),
        features: metadata.features.clone(),
        target: metadata.target.clone(),
        prediction_days: metadata.prediction_days,
        model_type: metadata.model_type.clone(),
        epochs: epochs as usize,
        batch_size: batch_size as usize,
        learning_rate,
        dropout: 0.2,
        train_test_split: 0.8,
    };
    
    println!("ğŸ”„ å¼€å§‹é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨æ‰©å±•çš„æ—¶é—´èŒƒå›´: {} åˆ° {}", 
             request.start_date, request.end_date);
    
    // åˆ é™¤æ—§æ¨¡å‹
    delete_model(&model_id).map_err(|e| format!("åˆ é™¤æ—§æ¨¡å‹å¤±è´¥: {}", e))?;
    
    // è®­ç»ƒæ–°æ¨¡å‹
    let result = train_candle_model(request).await?;
    
    println!("âœ… æ¨¡å‹é‡æ–°è®­ç»ƒå®Œæˆï¼Œæ–°å‡†ç¡®ç‡: {:.4}%", result.accuracy * 100.0);
    
    Ok(())
}

// è¯„ä¼°æ¨¡å‹
pub async fn evaluate_candle_model(model_id: String) -> std::result::Result<EvaluationResult, String> {
    // åŠ è½½æ¨¡å‹å…ƒæ•°æ®
    let metadata = load_model_metadata(&model_id)
        .map_err(|e| format!("åŠ è½½æ¨¡å‹å…ƒæ•°æ®å¤±è´¥: {}", e))?;
    
    // åŠ è½½æ¨¡å‹
    let device = Device::Cpu;
    
    let config = ModelConfig {
        model_type: metadata.model_type.clone(),
        input_size: metadata.features.len(),
        hidden_size: 64,
        output_size: 1,
        dropout: 0.0,
        learning_rate: 0.001,
        n_layers: 2,
        n_heads: 4,
        max_seq_len: 60,
    };
    
    let mut varmap = VarMap::new();
    let (_, model) = create_model(&config, &device)
        .map_err(|e| format!("æ¨¡å‹åˆ›å»ºå¤±è´¥: {}", e))?;
    
    let model_path = get_model_dir(&model_id).join("model.safetensors");
    varmap.load(&model_path).map_err(|e| format!("æ¨¡å‹åŠ è½½å¤±è´¥: {}", e))?;
    
    // åˆ›å»ºæµ‹è¯•æ•°æ® (åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”ä»æ•°æ®åº“è·å–å®é™…è‚¡ç¥¨æ•°æ®)
    // è¿™é‡Œè¿›è¡ŒåŸºæœ¬çš„æ¨¡æ‹Ÿï¼Œä»¥å±•ç¤ºè¯„ä¼°è¿‡ç¨‹
    
    // æ¨¡æ‹Ÿ100å¤©çš„å†å²æ•°æ®
    let days = 100;
    let mut price = 100.0;
    let mut prices = Vec::with_capacity(days);
    let mut volumes = Vec::with_capacity(days);
    
    for i in 0..days {
        // ç”Ÿæˆä»·æ ¼(æœ‰ä¸€å®šçš„éšæœºæ³¢åŠ¨å’Œè¶‹åŠ¿)
        let trend = ((i as f64 / 30.0).sin() * 10.0) + ((i as f64 / 90.0).cos() * 5.0);
        let random = (rand::random::<f64>() - 0.5) * 4.0;
        price = price + trend * 0.01 + random;
        prices.push(price);
        
        // ç”Ÿæˆæˆäº¤é‡
        let volume = (1000000.0 * (1.0 + (rand::random::<f64>() - 0.5) * 0.3)) as i64;
        volumes.push(volume);
    }
    
    // ç”Ÿæˆç‰¹å¾çŸ©é˜µ
    let mut features_matrix = Vec::new();
    for i in 19..days {  // ä»ç¬¬20å¤©å¼€å§‹ï¼Œä¿è¯æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®è®¡ç®—æŒ‡æ ‡
        let mut feature_vector = Vec::new();
        
        for feature_name in &metadata.features {
            match feature_name.as_str() {
                "close" => {
                    let normalized = (prices[i] - 95.0) / 20.0;
                    feature_vector.push(normalized);
                },
                "volume" => {
                    let normalized = volumes[i] as f64 / 2000000.0;
                    feature_vector.push(normalized);
                },
                "change_percent" => {
                    let change = (prices[i] - prices[i-1]) / prices[i-1];
                    feature_vector.push(change);
                },
                "ma5" => {
                    let ma5 = prices[i-4..=i].iter().sum::<f64>() / 5.0;
                    let normalized = (ma5 - prices[i]) / prices[i];
                    feature_vector.push(normalized);
                },
                "ma10" => {
                    let ma10 = prices[i-9..=i].iter().sum::<f64>() / 10.0;
                    let normalized = (ma10 - prices[i]) / prices[i];
                    feature_vector.push(normalized);
                },
                "ma20" => {
                    let ma20 = prices[i-19..=i].iter().sum::<f64>() / 20.0;
                    let normalized = (ma20 - prices[i]) / prices[i];
                    feature_vector.push(normalized);
                },
                "rsi" => {
                    // ç®€åŒ–çš„RSIè®¡ç®—
                    let gains = prices[i-14..i].iter()
                        .zip(prices[i-13..=i].iter())
                        .map(|(prev, curr)| {
                            let diff = curr - prev;
                            if diff > 0.0 { diff } else { 0.0 }
                        })
                        .sum::<f64>() / 14.0;
                        
                    let losses = prices[i-14..i].iter()
                        .zip(prices[i-13..=i].iter())
                        .map(|(prev, curr)| {
                            let diff = prev - curr;
                            if diff > 0.0 { diff } else { 0.0 }
                        })
                        .sum::<f64>() / 14.0;
                        
                    let rsi = if losses == 0.0 { 
                        100.0 
                    } else { 
                        100.0 - (100.0 / (1.0 + (gains / losses))) 
                    };
                    
                    feature_vector.push(rsi / 100.0);
                },
                "macd" => {
                    // ç®€åŒ–çš„MACDè®¡ç®—
                    let ema12 = prices[i-11..=i].iter().sum::<f64>() / 12.0;
                    let ema26 = prices[i-25..=i].iter().sum::<f64>() / 26.0;
                    let macd = ema12 - ema26;
                    let normalized = macd / prices[i];
                    feature_vector.push(normalized);
                },
                "bollinger" => {
                    // å¸ƒæ—å¸¦è®¡ç®—
                    let ma20 = prices[i-19..=i].iter().sum::<f64>() / 20.0;
                    let variance = prices[i-19..=i]
                        .iter()
                        .map(|p| (p - ma20).powi(2))
                        .sum::<f64>() / 20.0;
                    let std_dev = variance.sqrt();
                    let upper_band = ma20 + 2.0 * std_dev;
                    let lower_band = ma20 - 2.0 * std_dev;
                    
                    // è®¡ç®—ä»·æ ¼åœ¨å¸ƒæ—å¸¦ä¸­çš„ç›¸å¯¹ä½ç½® (-1åˆ°1)
                    let position = if upper_band == lower_band {
                        0.0
                    } else {
                        2.0 * (prices[i] - lower_band) / (upper_band - lower_band) - 1.0
                    };
                    
                    feature_vector.push(position);
                },
                "stochastic_k" => {
                    // Kå€¼è®¡ç®—
                    let highest_high = prices[i-13..=i].iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
                    let lowest_low = prices[i-13..=i].iter().fold(f64::INFINITY, |a, &b| a.min(b));
                    
                    let k = if highest_high == lowest_low {
                        0.5
                    } else {
                        (prices[i] - lowest_low) / (highest_high - lowest_low)
                    };
                    
                    feature_vector.push(k);
                },
                "stochastic_d" => {
                    // Kå€¼è®¡ç®—
                    let highest_high = prices[i-13..=i].iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
                    let lowest_low = prices[i-13..=i].iter().fold(f64::INFINITY, |a, &b| a.min(b));
                    
                    let k1 = if highest_high == lowest_low {
                        0.5
                    } else {
                        (prices[i] - lowest_low) / (highest_high - lowest_low)
                    };
                    
                    let k2 = if highest_high == lowest_low {
                        0.5
                    } else {
                        (prices[i-1] - lowest_low) / (highest_high - lowest_low)
                    };
                    
                    let k3 = if highest_high == lowest_low {
                        0.5
                    } else {
                        (prices[i-2] - lowest_low) / (highest_high - lowest_low)
                    };
                    
                    let d = (k1 + k2 + k3) / 3.0;
                    feature_vector.push(d);
                },
                "momentum" => {
                    // åŠ¨é‡æŒ‡æ ‡(å½“å‰ä»·æ ¼ä¸10æ—¥å‰ä»·æ ¼çš„æ¯”ç‡)
                    if i >= 29 {
                        let momentum = prices[i] / prices[i-10] - 1.0;
                        feature_vector.push(momentum);
                    } else {
                        feature_vector.push(0.0);
                    }
                },
                _ => {
                    // æœªçŸ¥ç‰¹å¾ï¼Œæ·»åŠ 0å€¼
                    feature_vector.push(0.0);
                }
            }
        }
        
        features_matrix.push(feature_vector);
    }
    
    // ç”Ÿæˆç›®æ ‡å˜é‡: ä½¿ç”¨5å¤©åçš„ä»·æ ¼å˜åŒ–ç‡
    let pred_days = metadata.prediction_days;
    let mut targets = Vec::new();
    
    // ä»·æ ¼ä»ç¬¬20å¤©å¼€å§‹
    let prices_offset = 20;
    
    for i in 0..features_matrix.len() - pred_days {
        let current_price = prices[i + prices_offset];
        let future_price = prices[i + prices_offset + pred_days];
        
        // è®¡ç®—æœªæ¥ä»·æ ¼ç›¸å¯¹å½“å‰ä»·æ ¼çš„å˜åŒ–ç‡
        let change_rate = (future_price - current_price) / current_price;
        
        // å°†å˜åŒ–ç‡åˆ†ç±»ä¸ºä¸Šæ¶¨/ä¸‹è·Œ/æŒå¹³
        let target = if change_rate > 0.01 {
            1.0  // æ˜æ˜¾ä¸Šæ¶¨
        } else if change_rate < -0.01 {
            -1.0 // æ˜æ˜¾ä¸‹è·Œ
        } else {
            0.0  // åŸºæœ¬æŒå¹³
        };
        
        targets.push(target);
    }
    
    // æˆªæ–­ç‰¹å¾çŸ©é˜µï¼Œä½¿å…¶ä¸ç›®æ ‡å˜é‡é•¿åº¦åŒ¹é…
    features_matrix.truncate(targets.len());
    
    // å°†ç‰¹å¾å’Œç›®æ ‡è½¬æ¢ä¸ºå¼ é‡ï¼Œä½¿ç”¨F32ç±»å‹ä»¥åŒ¹é…æ¨¡å‹æƒé‡
    let features_len = features_matrix[0].len();
    let test_size = features_matrix.len();
    
    let x_test_vec: Vec<f64> = features_matrix.iter()
        .flat_map(|v| v.iter().copied())
        .collect();
    
    let x_test_f32: Vec<f32> = x_test_vec.iter().map(|&x| x as f32).collect();
    let x_test = Tensor::from_slice(&x_test_f32, &[test_size, features_len], &device)
        .map_err(|e| format!("åˆ›å»ºæµ‹è¯•ç‰¹å¾å¼ é‡å¤±è´¥: {}", e))?;
    
    // è¿›è¡Œé¢„æµ‹
    let y_pred = model.forward(&x_test)
        .map_err(|e| format!("é¢„æµ‹å¤±è´¥: {}", e))?;
    
    // æå–é¢„æµ‹ç»“æœ
    let y_pred_vec = y_pred.flatten(0, 1).map_err(|e| format!("å±•å¹³é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?
        .to_vec1::<f32>().map_err(|e| format!("è½¬æ¢é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?;
    
    // è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    let mut correct = 0;
    let mut confusion_matrix = vec![vec![0; 3]; 3];  // 3x3çŸ©é˜µ: [ä¸‹è·Œ, æŒå¹³, ä¸Šæ¶¨]
    
    for (i, &pred_val) in y_pred_vec.iter().enumerate() {
        let actual_val = targets[i];
        
        // å°†é¢„æµ‹çš„å˜åŒ–ç‡è½¬æ¢ä¸ºç±»åˆ«
        let pred_class = if pred_val > 0.01 {
            1.0  // ä¸Šæ¶¨
        } else if pred_val < -0.01 {
            -1.0 // ä¸‹è·Œ
        } else {
            0.0  // æŒå¹³
        };
        
        // æ›´æ–°æ­£ç¡®é¢„æµ‹è®¡æ•°
        if (pred_class > 0.0 && actual_val > 0.0) || 
           (pred_class < 0.0 && actual_val < 0.0) || 
           (pred_class == 0.0 && actual_val == 0.0) {
            correct += 1;
        }
        
        // æ›´æ–°æ··æ·†çŸ©é˜µ
        let actual_idx = match actual_val {
            -1.0 => 0, // ä¸‹è·Œ
            0.0 => 1,  // æŒå¹³
            _ => 2,    // ä¸Šæ¶¨
        };
        
        let pred_idx = match pred_class {
            -1.0 => 0, // ä¸‹è·Œ
            0.0 => 1,  // æŒå¹³
            _ => 2,    // ä¸Šæ¶¨
        };
        
        confusion_matrix[actual_idx][pred_idx] += 1;
    }
    
    // è®¡ç®—å‡†ç¡®ç‡
    let accuracy = correct as f64 / targets.len() as f64;
    
    // è®¡ç®—F1åˆ†æ•°ã€ç²¾ç¡®ç‡å’Œå¬å›ç‡(é’ˆå¯¹ä¸Šæ¶¨ç±»åˆ«)
    let true_positives = confusion_matrix[2][2]; // å®é™…ä¸Šæ¶¨ï¼Œé¢„æµ‹ä¸Šæ¶¨
    let false_positives = confusion_matrix[0][2] + confusion_matrix[1][2]; // å®é™…éä¸Šæ¶¨ï¼Œé¢„æµ‹ä¸Šæ¶¨
    let false_negatives = confusion_matrix[2][0] + confusion_matrix[2][1]; // å®é™…ä¸Šæ¶¨ï¼Œé¢„æµ‹éä¸Šæ¶¨
    
    let precision = if true_positives + false_positives > 0 {
        true_positives as f64 / (true_positives + false_positives) as f64
    } else {
        0.0
    };
    
    let recall = if true_positives + false_negatives > 0 {
        true_positives as f64 / (true_positives + false_negatives) as f64
    } else {
        0.0
    };
    
    let f1_score = if precision + recall > 0.0 {
        2.0 * precision * recall / (precision + recall)
    } else {
        0.0
    };
    
    // ä¿å­˜ä¸€äº›é¢„æµ‹ç¤ºä¾‹
    let mut prediction_examples = Vec::new();
    let max_examples = std::cmp::min(10, targets.len());
    
    for i in 0..max_examples {
        prediction_examples.push(PredictionExample {
            actual: targets[i],
            predicted: y_pred_vec[i] as f64,
            features: features_matrix[i].clone(),
        });
    }
    
    // è¿”å›è¯„ä¼°ç»“æœ
    Ok(EvaluationResult {
        accuracy,
        confusion_matrix,
        precision,
        recall,
        f1_score,
        prediction_examples,
    })
}

// å¯¼å‡ºçš„å‘½ä»¤
pub async fn list_stock_prediction_models(symbol: String) -> Vec<ModelInfo> {
    list_models(&symbol)
}

pub async fn delete_stock_prediction_model(model_id: String) -> std::result::Result<(), String> {
    delete_model(&model_id).map_err(|e| format!("åˆ é™¤æ¨¡å‹å¤±è´¥: {}", e))
}

// Aè‚¡äº¤æ˜“è§„åˆ™å·¥å…·å‡½æ•°
fn is_trading_day(date: chrono::NaiveDate) -> bool {
    match date.weekday() {
        Weekday::Mon | Weekday::Tue | Weekday::Wed | Weekday::Thu | Weekday::Fri => {
            // è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥æ·»åŠ èŠ‚å‡æ—¥åˆ¤æ–­
            // æš‚æ—¶åªåˆ¤æ–­å·¥ä½œæ—¥
            true
        },
        _ => false
    }
}

fn get_next_trading_day(date: chrono::NaiveDate) -> chrono::NaiveDate {
    let mut next_date = date + chrono::Duration::days(1);
    while !is_trading_day(next_date) {
        next_date += chrono::Duration::days(1);
    }
    next_date
}

fn clamp_daily_change(change_percent: f64) -> f64 {
    // Aè‚¡æ¶¨è·Œåœé™åˆ¶ï¼šÂ±10%
    change_percent.clamp(-10.0, 10.0)
}

// è®¡ç®—å†å²æ³¢åŠ¨ç‡
fn calculate_historical_volatility(prices: &[f64]) -> f64 {
    if prices.len() < 20 {
        return 0.02; // é»˜è®¤2%æ³¢åŠ¨ç‡
    }
    
    // è®¡ç®—è¿‡å»20å¤©çš„ä»·æ ¼å˜åŒ–ç‡
    let mut daily_returns = Vec::new();
    for i in 1..std::cmp::min(21, prices.len()) {
        let return_rate = (prices[prices.len() - i] - prices[prices.len() - i - 1]) / prices[prices.len() - i - 1];
        daily_returns.push(return_rate);
    }
    
    // è®¡ç®—æ ‡å‡†å·®
    let mean = daily_returns.iter().sum::<f64>() / daily_returns.len() as f64;
    let variance = daily_returns.iter()
        .map(|x| (x - mean).powi(2))
        .sum::<f64>() / daily_returns.len() as f64;
    
    variance.sqrt().min(0.1) // é™åˆ¶æœ€å¤§æ³¢åŠ¨ç‡ä¸º10%
}

// è®¡ç®—è¿‘æœŸè¶‹åŠ¿
fn calculate_recent_trend(prices: &[f64]) -> f64 {
    if prices.len() < 10 {
        return 0.0;
    }
    
    let recent_len = std::cmp::min(10, prices.len());
    let recent_prices = &prices[prices.len() - recent_len..];
    
    // ä½¿ç”¨ç®€å•çº¿æ€§å›å½’è®¡ç®—è¶‹åŠ¿
    let n = recent_len as f64;
    let sum_x = (0..recent_len).sum::<usize>() as f64;
    let sum_y = recent_prices.iter().sum::<f64>();
    let sum_xy = recent_prices.iter().enumerate()
        .map(|(i, &price)| i as f64 * price)
        .sum::<f64>();
    let sum_x2 = (0..recent_len).map(|i| (i * i) as f64).sum::<f64>();
    
    // è¶‹åŠ¿æ–œç‡
    let slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x);
    let avg_price = sum_y / n;
    
    // æ ‡å‡†åŒ–è¶‹åŠ¿ (ç›¸å¯¹äºå¹³å‡ä»·æ ¼)
    (slope / avg_price).clamp(-0.05, 0.05) // é™åˆ¶åœ¨Â±5%èŒƒå›´å†…
}

// è®¡ç®—æ”¯æ’‘é˜»åŠ›ä½
fn calculate_support_resistance(prices: &[f64], current_price: f64) -> f64 {
    if prices.len() < 30 {
        return 0.01; // é»˜è®¤1%å½±å“
    }
    
    let recent_len = std::cmp::min(30, prices.len());
    let recent_prices = &prices[prices.len() - recent_len..];
    
    // å¯»æ‰¾å±€éƒ¨é«˜ç‚¹å’Œä½ç‚¹
    let mut highs = Vec::new();
    let mut lows = Vec::new();
    
    for i in 1..recent_prices.len() - 1 {
        if recent_prices[i] > recent_prices[i-1] && recent_prices[i] > recent_prices[i+1] {
            highs.push(recent_prices[i]);
        }
        if recent_prices[i] < recent_prices[i-1] && recent_prices[i] < recent_prices[i+1] {
            lows.push(recent_prices[i]);
        }
    }
    
    // æ‰¾åˆ°æœ€è¿‘çš„æ”¯æ’‘ä½å’Œé˜»åŠ›ä½
    let resistance = highs.iter().fold(0.0, |acc, &x| if x > current_price && (acc == 0.0 || x < acc) { x } else { acc });
    let support = lows.iter().fold(0.0, |acc, &x| if x < current_price && x > acc { x } else { acc });
    
    // è®¡ç®—æ”¯æ’‘é˜»åŠ›å½±å“
    let sr_strength = if resistance > 0.0 && support > 0.0 {
        let resistance_dist = (resistance - current_price) / current_price;
        let support_dist = (current_price - support) / current_price;
        (resistance_dist - support_dist) * 0.5 // å¹³è¡¡æ”¯æ’‘é˜»åŠ›å½±å“
    } else if resistance > 0.0 {
        (resistance - current_price) / current_price * 0.3
    } else if support > 0.0 {
        (current_price - support) / current_price * 0.3
    } else {
        0.0
    };
    
    sr_strength.clamp(-0.03, 0.03) // é™åˆ¶åœ¨Â±3%èŒƒå›´å†…
}

// è®¡ç®—æ›´çœŸå®çš„æ¨¡å‹å‡†ç¡®ç‡
fn calculate_realistic_accuracy(predictions: &[f64], actuals: &[f64]) -> f64 {
    if predictions.len() != actuals.len() || predictions.is_empty() {
        return 0.0;
    }
    
    let mut correct_direction = 0;
    let mut total_valid = 0;
    let mut mse_sum = 0.0;
    
    for i in 1..predictions.len() {
        let pred_change = predictions[i] - predictions[i-1];
        let actual_change = actuals[i] - actuals[i-1];
        
        // æ–¹å‘å‡†ç¡®æ€§ (æ¶¨è·Œæ–¹å‘æ˜¯å¦ä¸€è‡´)
        if (pred_change > 0.0 && actual_change > 0.0) || 
           (pred_change < 0.0 && actual_change < 0.0) ||
           (pred_change.abs() < 0.01 && actual_change.abs() < 0.01) {
            correct_direction += 1;
        }
        
        // MSEè®¡ç®—
        let error = (predictions[i] - actuals[i]) / actuals[i]; // ç›¸å¯¹è¯¯å·®
        mse_sum += error * error;
        total_valid += 1;
    }
    
    if total_valid == 0 {
        return 0.0;
    }
    
    let direction_accuracy = correct_direction as f64 / total_valid as f64;
    let mse = mse_sum / total_valid as f64;
    let price_accuracy = (1.0 - mse.sqrt()).max(0.0); // åŸºäºRMSEçš„ä»·æ ¼å‡†ç¡®æ€§
    
    // ç»¼åˆå‡†ç¡®ç‡ï¼šæ–¹å‘å‡†ç¡®æ€§æƒé‡0.6ï¼Œä»·æ ¼å‡†ç¡®æ€§æƒé‡0.4
    let combined_accuracy = direction_accuracy * 0.6 + price_accuracy * 0.4;
    
    // å¯¹äºè‚¡ç¥¨é¢„æµ‹ï¼Œ50%ä»¥ä¸Šå°±æ˜¯ä¸é”™çš„å‡†ç¡®ç‡ï¼Œ70%ä»¥ä¸Šæ˜¯å¾ˆå¥½çš„
    // é™åˆ¶æœ€é«˜å‡†ç¡®ç‡ä»¥ä¿æŒç°å®æ€§
    combined_accuracy.min(0.75)
}
