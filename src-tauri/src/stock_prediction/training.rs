use candle_core::{Device, Tensor};
use candle_nn::{Module, Optimizer, VarMap, AdamW};
use std::fs;
use chrono;
use crate::stock_prediction::types::{
    ModelConfig, ModelInfo, TrainingRequest, TrainingResult
};
use crate::stock_prediction::database::get_historical_data_from_db;
use crate::stock_prediction::utils::{
    smooth_price_data, smooth_volume_data, 
    calculate_direction_focused_accuracy
};
use crate::stock_prediction::technical_indicators::{
    get_feature_required_days, calculate_feature_value
};
use crate::stock_prediction::model_management::{
    save_model_metadata, generate_model_id, get_current_timestamp, get_model_file_path
};

// ç®€åŒ–çš„æ¨¡å‹åˆ›å»ºå‡½æ•°
fn create_model(config: &ModelConfig, device: &Device) -> Result<(VarMap, Box<dyn Module + Send + Sync>), candle_core::Error> {
    // åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹
    let varmap = VarMap::new();
    
    // å®šä¹‰æ­£ç¡®çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶
    let input_size = config.input_size;
    let output_size = config.output_size;
    
    struct LinearRegression {
        linear: candle_nn::Linear,
    }
    
    impl LinearRegression {
        fn new(in_size: usize, out_size: usize, vb: candle_nn::VarBuilder) -> Result<Self, candle_core::Error> {
            let linear = candle_nn::linear(in_size, out_size, vb)?;
            Ok(Self { linear })
        }
    }
    
    unsafe impl Send for LinearRegression {}
    unsafe impl Sync for LinearRegression {}
    
    impl Module for LinearRegression {
        fn forward(&self, xs: &Tensor) -> Result<Tensor, candle_core::Error> {
            self.linear.forward(xs)
        }
    }
    
    let vb = candle_nn::VarBuilder::from_varmap(&varmap, candle_core::DType::F32, device);
    let model = LinearRegression::new(input_size, output_size, vb)?;
    
    let model: Box<dyn Module + Send + Sync> = Box::new(model);
    
    Ok((varmap, model))
}

// ç®€åŒ–çš„æ¨¡å‹ä¿å­˜å‡½æ•°
fn save_model(varmap: &VarMap, path: &std::path::Path) -> Result<(), candle_core::Error> {
    varmap.save(path)?;
    Ok(())
}

// æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†å‡½æ•°
async fn prepare_stock_data(
    request: &TrainingRequest,
) -> std::result::Result<(Tensor, Tensor, Tensor, Tensor, Vec<String>), candle_core::Error> {
    // è®¾ç½®è®¾å¤‡
    let device = Device::Cpu;
    
    let symbol = &request.stock_code;
    
    // è§£æå‰ç«¯ä¼ æ¥çš„æ—¥æœŸèŒƒå›´
    let end_date = chrono::Local::now().naive_local().date();
    let user_start_date = chrono::NaiveDate::parse_from_str(&request.start_date, "%Y-%m-%d")
        .unwrap_or_else(|_| end_date - chrono::Duration::days(210)); // é»˜è®¤210å¤©
    let user_end_date = chrono::NaiveDate::parse_from_str(&request.end_date, "%Y-%m-%d")
        .unwrap_or(end_date);
    
    // è®¡ç®—ç”¨æˆ·è¯·æ±‚çš„å¤©æ•°èŒƒå›´
    let requested_days = (user_end_date - user_start_date).num_days();
    
    // ä¸ºAè‚¡èŠ‚å‡æ—¥å¢åŠ é¢å¤–ç¼“å†²æœŸ
    // å¦‚æœç”¨æˆ·å·²ç»åŒ…å«äº†ç¼“å†²æœŸï¼ˆå¦‚180+30=210å¤©ï¼‰ï¼Œæˆ‘ä»¬å†å¢åŠ ä¸€äº›ä»¥ç¡®ä¿æ•°æ®å……è¶³
    let additional_buffer = if requested_days >= 200 { 
        60  // ç”¨æˆ·å·²æœ‰ç¼“å†²ï¼Œå†åŠ 60å¤©
    } else { 
        90  // ç”¨æˆ·æ²¡æœ‰ç¼“å†²ï¼ŒåŠ 90å¤©
    };
    
    let extended_start_date = user_start_date - chrono::Duration::days(additional_buffer);
    
    // ç¡®ä¿ä¸ä¼šæŸ¥è¯¢è¿‡äºä¹…è¿œçš„æ•°æ®ï¼ˆæœ€å¤š2å¹´ï¼‰
    let max_start_date = end_date - chrono::Duration::days(730);
    let actual_start_date = if extended_start_date < max_start_date {
        max_start_date
    } else {
        extended_start_date
    };
    
    let start_date_str = actual_start_date.format("%Y-%m-%d").to_string();
    let end_date_str = user_end_date.format("%Y-%m-%d").to_string();
    
    println!("ğŸ“… Aè‚¡æ•°æ®è·å–ç­–ç•¥:");
    println!("   ç”¨æˆ·è¯·æ±‚èŒƒå›´: {} åˆ° {} ({} å¤©)", 
             user_start_date.format("%Y-%m-%d"), 
             user_end_date.format("%Y-%m-%d"), 
             requested_days);
    println!("   å®é™…æŸ¥è¯¢èŒƒå›´: {} åˆ° {} ({} å¤©ï¼Œå«èŠ‚å‡æ—¥ç¼“å†²)", 
             start_date_str, end_date_str, 
             (user_end_date - actual_start_date).num_days());
    
    // ä½¿ç”¨sqlxæŸ¥è¯¢æ•°æ®åº“è·å–å†å²æ•°æ®
    let historical_data = match get_historical_data_from_db(symbol, &start_date_str, &end_date_str).await {
        Ok(data) => data,
        Err(e) => {
            eprintln!("ä»æ•°æ®åº“è·å–æ•°æ®å¤±è´¥: {}", e);
            return Err(candle_core::Error::Msg(format!("è·å–å†å²æ•°æ®å¤±è´¥: {}", e)));
        }
    };
    
    if historical_data.is_empty() {
        return Err(candle_core::Error::Msg("å†å²æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹".to_string()));
    }
    
    println!("âœ… è·å–åˆ°{}æ¡å†å²æ•°æ®", historical_data.len());
    
    // æ•°æ®è´¨é‡æ£€æŸ¥ - é’ˆå¯¹Aè‚¡ç‰¹ç‚¹ä¼˜åŒ–
    let valid_data: Vec<_> = historical_data.into_iter()
        .filter(|data| {
            // Aè‚¡åŸºæœ¬æ•°æ®éªŒè¯
            data.close > 0.0 && data.volume >= 0 && 
            data.open > 0.0 && data.high > 0.0 && data.low > 0.0 &&
            data.high >= data.low && data.high >= data.open && 
            data.high >= data.close && data.low <= data.open && data.low <= data.close &&
            // Aè‚¡æ¶¨è·Œå¹…é™åˆ¶æ£€æŸ¥ï¼ˆSTè‚¡ç¥¨20%ï¼Œæ™®é€šè‚¡ç¥¨10%ï¼‰
            data.change_percent.abs() <= 25.0 && // å…è®¸ä¸€äº›æ•°æ®è¯¯å·®
            // æˆäº¤é‡åˆç†æ€§æ£€æŸ¥
            data.volume < 1_000_000_000_000 // é¿å…å¼‚å¸¸å¤§çš„æˆäº¤é‡
        })
        .collect();
    
    println!("âœ… è¿‡æ»¤åæœ‰æ•ˆæ•°æ®{}æ¡", valid_data.len());
    
    // Aè‚¡äº¤æ˜“æ—¥æ•°é‡ä¼°ç®—ï¼šä¸€å¹´çº¦250ä¸ªäº¤æ˜“æ—¥
    let min_required_days = 120; // æœ€å°‘çº¦åŠå¹´äº¤æ˜“æ•°æ®
    let recommended_days = 180; // æ¨èçº¦9ä¸ªæœˆäº¤æ˜“æ•°æ®
    let optimal_days = 250; // æœ€ä½³çº¦1å¹´äº¤æ˜“æ•°æ®
    
    if valid_data.len() < min_required_days {
        return Err(candle_core::Error::Msg(format!(
            "Aè‚¡æœ‰æ•ˆäº¤æ˜“æ•°æ®ä¸è¶³ï¼Œå½“å‰{}å¤©ï¼Œéœ€è¦è‡³å°‘{}å¤©æ•°æ®ï¼ˆçº¦åŠå¹´äº¤æ˜“æ—¥ï¼‰", 
            valid_data.len(), min_required_days
        )));
    }
    
    if valid_data.len() < recommended_days {
        println!("âš ï¸  è­¦å‘Š: å½“å‰æ•°æ®é‡{}å¤©å°‘äºæ¨èçš„{}å¤©ï¼Œå¯èƒ½å½±å“æ¨¡å‹å‡†ç¡®ç‡", 
                 valid_data.len(), recommended_days);
    } else if valid_data.len() >= optimal_days {
        println!("âœ… æ•°æ®é‡å……è¶³: {}å¤© >= {}å¤©ï¼Œæœ‰åˆ©äºæé«˜æ¨¡å‹å‡†ç¡®ç‡", 
                 valid_data.len(), optimal_days);
    }
    
    // æ„å»ºç‰¹å¾å’Œæ ‡ç­¾
    let mut dates = Vec::new();
    let mut prices = Vec::new();
    let mut volumes = Vec::new();
    let mut highs = Vec::new();
    let mut lows = Vec::new();
    let mut opens = Vec::new();
    let mut features_matrix = Vec::new();
    
    // æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
    let mut sorted_data = valid_data.clone();
    sorted_data.sort_by(|a, b| a.date.cmp(&b.date));
    
    // é¦–å…ˆæå–åŸºç¡€æ•°æ®
    for data in &sorted_data {
        dates.push(data.date.clone());
        prices.push(data.close);
        volumes.push(data.volume);
        highs.push(data.high);
        lows.push(data.low);
        opens.push(data.open);
    }
    
    // æ•°æ®å¹³æ»‘å¤„ç†ï¼šç§»é™¤å¼‚å¸¸å€¼
    let prices = smooth_price_data(&prices);
    let volumes = smooth_volume_data(&volumes);
    
    // ä¸ºæ¯å¤©å‡†å¤‡ä¸€ä¸ªç‰¹å¾å‘é‡ï¼ˆåŠ¨æ€è®¡ç®—æ‰€éœ€çš„å†å²çª—å£ï¼‰
    let required_days = request.features.iter()
        .map(|f| get_feature_required_days(f))
        .max()
        .unwrap_or(20);
    
    // ä½¿ç”¨è¾ƒå°çš„lookback_windowï¼Œä½†ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è®¡ç®—æ‰€æœ‰ç‰¹å¾
    let lookback_window = required_days.max(30).min(prices.len() / 2);
    
    println!("ğŸ“Š ç‰¹å¾è®¡ç®—çª—å£: {}å¤©, æ€»ä»·æ ¼æ•°æ®: {}å¤©", lookback_window, prices.len());
    
    for i in lookback_window..prices.len() {
        let mut feature_vector = Vec::new();
        
        // æå–è¯·æ±‚ä¸­æŒ‡å®šçš„ç‰¹å¾
        for feature_name in &request.features {
            let feature_value = calculate_feature_value(
                feature_name, 
                &prices, 
                &volumes, 
                i, 
                lookback_window,
                Some(&highs),
                Some(&lows)
            )?;
            feature_vector.push(feature_value);
        }
        
        features_matrix.push(feature_vector);
    }
    
    println!("ğŸ”¢ ç”Ÿæˆç‰¹å¾çŸ©é˜µ: {}è¡Œ x {}åˆ—", features_matrix.len(), 
             if features_matrix.is_empty() { 0 } else { features_matrix[0].len() });
    
    // ç§»é™¤å‰é¢çš„æ•°æ®ï¼Œå› ä¸ºæ²¡æœ‰è®¡ç®—ç‰¹å¾
    dates = dates[lookback_window..].to_vec();
    let valid_prices = prices[lookback_window..].to_vec();
    
    // åˆ›å»ºç›®æ ‡å˜é‡: ä½¿ç”¨æœªæ¥nå¤©çš„ä»·æ ¼å˜åŒ–ç‡
    let pred_days = request.prediction_days;
    let mut targets = Vec::new();
    
    // é˜²æ­¢æ•´æ•°æº¢å‡ºï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œé¢„æµ‹
    if features_matrix.len() <= pred_days {
        return Err(candle_core::Error::Msg(format!(
            "æ•°æ®ä¸è¶³ä»¥è¿›è¡Œ{}å¤©é¢„æµ‹ï¼Œå½“å‰ç‰¹å¾æ•°æ®{}å¤©ï¼Œéœ€è¦è‡³å°‘{}å¤©", 
            pred_days, features_matrix.len(), pred_days + 1
        )));
    }
    
    for i in 0..(features_matrix.len() - pred_days) {
        let current_price = valid_prices[i];
        
        // è®¡ç®—æœªæ¥å‡ å¤©çš„å¹³å‡ä»·æ ¼ï¼Œå‡å°‘å™ªéŸ³
        let future_prices: Vec<f64> = (1..=pred_days)
            .map(|day| valid_prices.get(i + day).copied().unwrap_or(current_price))
            .collect();
        
        let future_avg_price = future_prices.iter().sum::<f64>() / future_prices.len() as f64;
        let change_rate = (future_avg_price - current_price) / current_price;
        
        // é™åˆ¶å˜åŒ–ç‡èŒƒå›´ï¼Œé¿å…æç«¯å€¼å½±å“è®­ç»ƒ
        let clamped_change_rate = change_rate.clamp(-0.5, 0.5);
        targets.push(clamped_change_rate);
    }
    
    // æˆªæ–­ç‰¹å¾çŸ©é˜µï¼Œä½¿å…¶ä¸ç›®æ ‡å˜é‡é•¿åº¦åŒ¹é…
    features_matrix.truncate(targets.len());
    dates.truncate(targets.len());
    
    // ç‰¹å¾æ ‡å‡†åŒ–
    let (normalized_features, _feature_stats) = normalize_features(&features_matrix)?;
    
    // åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† - ä½¿ç”¨æ—¶é—´åºåˆ—åˆ’åˆ†
    let train_size = (targets.len() as f64 * request.train_test_split) as usize;
    
    // è½¬æ¢ä¸ºå¼ é‡
    let features_len = normalized_features[0].len();
    let x_train_vec: Vec<f64> = normalized_features[0..train_size].iter()
        .flat_map(|v| v.iter().copied())
        .collect();
    
    let y_train_vec: Vec<f64> = targets[0..train_size].to_vec();
    
    let x_test_vec: Vec<f64> = normalized_features[train_size..].iter()
        .flat_map(|v| v.iter().copied())
        .collect();
    
    let y_test_vec: Vec<f64> = targets[train_size..].to_vec();
    
    // åˆ›å»ºå¼ é‡ï¼Œä½¿ç”¨F32ç±»å‹ä»¥åŒ¹é…æ¨¡å‹æƒé‡
    let x_train_f32: Vec<f32> = x_train_vec.iter().map(|&x| x as f32).collect();
    let y_train_f32: Vec<f32> = y_train_vec.iter().map(|&y| y as f32).collect();
    let x_test_f32: Vec<f32> = x_test_vec.iter().map(|&x| x as f32).collect();
    let y_test_f32: Vec<f32> = y_test_vec.iter().map(|&y| y as f32).collect();
    
    let x_train = Tensor::from_slice(&x_train_f32, &[train_size, features_len], &device)?;
    let y_train = Tensor::from_slice(&y_train_f32, &[train_size, 1], &device)?;
    
    let test_size = targets.len() - train_size;
    let x_test = Tensor::from_slice(&x_test_f32, &[test_size, features_len], &device)?;
    let y_test = Tensor::from_slice(&y_test_f32, &[test_size, 1], &device)?;
    
    println!("æ•°æ®é¢„å¤„ç†å®Œæˆ: è®­ç»ƒé›†{}æ ·æœ¬, æµ‹è¯•é›†{}æ ·æœ¬, ç‰¹å¾ç»´åº¦{}", 
             train_size, test_size, features_len);
    
    Ok((x_train, y_train, x_test, y_test, dates))
}

// ç‰¹å¾æ ‡å‡†åŒ–
fn normalize_features(features: &[Vec<f64>]) -> Result<(Vec<Vec<f64>>, Vec<(f64, f64)>), candle_core::Error> {
    if features.is_empty() {
        return Ok((Vec::new(), Vec::new()));
    }
    
    let feature_count = features[0].len();
    let mut stats = Vec::with_capacity(feature_count);
    let mut normalized = vec![vec![0.0; feature_count]; features.len()];
    
    // è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®
    for feature_idx in 0..feature_count {
        let values: Vec<f64> = features.iter().map(|row| row[feature_idx]).collect();
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        let variance = values.iter()
            .map(|v| (v - mean).powi(2))
            .sum::<f64>() / values.len() as f64;
        let std_dev = variance.sqrt().max(1e-8); // é¿å…é™¤é›¶
        
        stats.push((mean, std_dev));
        
        // æ ‡å‡†åŒ–è¯¥ç‰¹å¾
        for (row_idx, row) in features.iter().enumerate() {
            normalized[row_idx][feature_idx] = (row[feature_idx] - mean) / std_dev;
        }
    }
    
    Ok((normalized, stats))
}

// è®­ç»ƒæ¨¡å‹å‡½æ•°
pub async fn train_candle_model(request: TrainingRequest) -> std::result::Result<TrainingResult, String> {
    let model_id = generate_model_id();
    let model_type = request.model_type.clone();
    
    // å‡†å¤‡æ•°æ®
    let (x_train, y_train, x_test, y_test, _) = prepare_stock_data(&request).await
        .map_err(|e| format!("æ•°æ®å‡†å¤‡å¤±è´¥: {}", e))?;
    
    // è®¾ç½®è®¾å¤‡
    let device = Device::Cpu;
    
    // åˆ›å»ºæ¨¡å‹é…ç½®
    let config = ModelConfig {
        model_type: model_type.clone(),
        input_size: request.features.len(),
        hidden_size: 64, // éšè—å±‚å¤§å°
        output_size: 1,  // è¾“å‡ºå°ºå¯¸ (è‚¡ä»·)
        dropout: request.dropout,
        learning_rate: request.learning_rate,
        n_layers: 2,     // é»˜è®¤å€¼
        n_heads: 4,      // é»˜è®¤å€¼
        max_seq_len: 60, // é»˜è®¤å€¼
    };
    
    // åˆ›å»ºæ¨¡å‹
    let (varmap, model) = create_model(&config, &device)
        .map_err(|e| format!("æ¨¡å‹åˆ›å»ºå¤±è´¥: {}", e))?;
    
    // åˆ›å»ºä¼˜åŒ–å™¨
    let mut optimizer = AdamW::new_lr(varmap.all_vars(), request.learning_rate)
        .map_err(|e| format!("ä¼˜åŒ–å™¨åˆ›å»ºå¤±è´¥: {}", e))?;
    
    // è®­ç»ƒæ¨¡å‹
    let batch_size = request.batch_size;
    let num_batches = x_train.dim(0).unwrap() / batch_size;
    
    for epoch in 0..request.epochs {
        let mut epoch_loss = 0.0;
        
        for batch_idx in 0..num_batches {
            let batch_start = batch_idx * batch_size;
            let x_batch = x_train.narrow(0, batch_start, batch_size)
                .map_err(|e| format!("æ‰¹æ¬¡æ•°æ®å‡†å¤‡å¤±è´¥: {}", e))?;
            let y_batch = y_train.narrow(0, batch_start, batch_size)
                .map_err(|e| format!("æ‰¹æ¬¡æ•°æ®å‡†å¤‡å¤±è´¥: {}", e))?;
            
            // å‰å‘ä¼ æ’­
            let output = model.forward(&x_batch)
                .map_err(|e| format!("å‰å‘ä¼ æ’­å¤±è´¥: {}", e))?;
            
            // è®¡ç®—æŸå¤± (å‡æ–¹è¯¯å·®)
            // ç¡®ä¿è¾“å‡ºå’Œç›®æ ‡å¼ é‡çš„å½¢çŠ¶åŒ¹é…
            println!("è¾“å‡ºå½¢çŠ¶: {:?}, ç›®æ ‡å½¢çŠ¶: {:?}", output.dims(), y_batch.dims());
            
            // å¦‚æœè¾“å‡ºå½¢çŠ¶å’Œç›®æ ‡å½¢çŠ¶ä¸åŒ¹é…ï¼Œåˆ™è¿›è¡Œè°ƒæ•´
            let reshaped_output = if output.dims() != y_batch.dims() {
                if output.dim(0).unwrap() == y_batch.dim(0).unwrap() {
                    // å¦‚æœæ‰¹æ¬¡å¤§å°ç›¸åŒä½†è¾“å‡ºç»´åº¦ä¸åŒï¼Œå°è¯•reshape
                    output.reshape(&[output.dim(0).unwrap(), 1])
                        .map_err(|e| format!("è°ƒæ•´è¾“å‡ºå½¢çŠ¶å¤±è´¥: {}", e))?
                } else {
                    return Err(format!("è¾“å‡ºå½¢çŠ¶ {:?} å’Œç›®æ ‡å½¢çŠ¶ {:?} ä¸å…¼å®¹", output.dims(), y_batch.dims()));
                }
            } else {
                output
            };
            
            let loss = reshaped_output.sub(&y_batch).map_err(|e| format!("è®¡ç®—æŸå¤±å¤±è´¥: {}", e))?;
            let loss_squared = loss.sqr().map_err(|e| format!("è®¡ç®—å¹³æ–¹å¤±è´¥: {}", e))?;
            let loss = loss_squared.mean_all().map_err(|e| format!("è®¡ç®—å‡å€¼å¤±è´¥: {}", e))?;
            
            // åå‘ä¼ æ’­
            optimizer.backward_step(&loss)
                .map_err(|e| format!("åå‘ä¼ æ’­å¤±è´¥: {}", e))?;
            
            epoch_loss += loss.to_scalar::<f32>().unwrap() as f64;
        }
        
        // æ¯10ä¸ªepochè®°å½•ä¸€æ¬¡æŸå¤±
        if (epoch + 1) % 10 == 0 || epoch == 0 || epoch == request.epochs - 1 {
            println!("Epoch {}/{}, Loss: {:.4}", epoch + 1, request.epochs, epoch_loss / num_batches as f64);
        }
    }
    
    // è¯„ä¼°æ¨¡å‹
    let y_pred = model.forward(&x_test)
        .map_err(|e| format!("é¢„æµ‹å¤±è´¥: {}", e))?;
    
    // è½¬æ¢ä¸ºVecç”¨äºå‡†ç¡®ç‡è®¡ç®— - å¤„ç†ä¸åŒç»´åº¦çš„å¼ é‡
    let predictions_vec = match y_pred.dims() {
        // å¦‚æœæ˜¯1ç»´å¼ é‡ [n]
        [_] => {
            y_pred.to_vec1::<f32>().map_err(|e| format!("è½¬æ¢1ç»´é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?
                .into_iter().map(|x| x as f64).collect::<Vec<f64>>()
        },
        // å¦‚æœæ˜¯2ç»´å¼ é‡ [n, 1] 
        [_, 1] => {
            y_pred.to_vec2::<f32>().map_err(|e| format!("è½¬æ¢2ç»´é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?
                .into_iter().map(|row| row[0] as f64).collect::<Vec<f64>>()
        },
        // å¦‚æœæ˜¯å…¶ä»–2ç»´å¼ é‡ [n, m]
        [_, _] => {
            let vec2d = y_pred.to_vec2::<f32>().map_err(|e| format!("è½¬æ¢2ç»´é¢„æµ‹ç»“æœå¤±è´¥: {}", e))?;
            vec2d.into_iter().map(|row| row[0] as f64).collect::<Vec<f64>>() // å–ç¬¬ä¸€åˆ—
        },
        // å…¶ä»–ç»´åº¦
        _ => {
            return Err("é¢„æµ‹è¾“å‡ºå¼ é‡ç»´åº¦ä¸æ”¯æŒï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®".to_string());
        }
    };
    
    let actuals_vec = match y_test.dims() {
        // å¦‚æœæ˜¯1ç»´å¼ é‡ [n]
        [_] => {
            y_test.to_vec1::<f32>().map_err(|e| format!("è½¬æ¢1ç»´å®é™…ç»“æœå¤±è´¥: {}", e))?
                .into_iter().map(|x| x as f64).collect::<Vec<f64>>()
        },
        // å¦‚æœæ˜¯2ç»´å¼ é‡ [n, 1]
        [_, 1] => {
            y_test.to_vec2::<f32>().map_err(|e| format!("è½¬æ¢2ç»´å®é™…ç»“æœå¤±è´¥: {}", e))?
                .into_iter().map(|row| row[0] as f64).collect::<Vec<f64>>()
        },
        // å¦‚æœæ˜¯å…¶ä»–2ç»´å¼ é‡ [n, m]
        [_, _] => {
            let vec2d = y_test.to_vec2::<f32>().map_err(|e| format!("è½¬æ¢2ç»´å®é™…ç»“æœå¤±è´¥: {}", e))?;
            vec2d.into_iter().map(|row| row[0] as f64).collect::<Vec<f64>>() // å–ç¬¬ä¸€åˆ—
        },
        // å…¶ä»–ç»´åº¦
        _ => {
            return Err("å®é™…å€¼å¼ é‡ç»´åº¦ä¸æ”¯æŒï¼Œè¯·æ£€æŸ¥æ•°æ®å‡†å¤‡".to_string());
        }
    };
    
    // ä½¿ç”¨æ”¹è¿›çš„å‡†ç¡®ç‡è®¡ç®—æ–¹æ³•ï¼ˆæ›´é‡è§†æ–¹å‘é¢„æµ‹ï¼‰
    let (direction_accuracy, combined_accuracy) = calculate_direction_focused_accuracy(&predictions_vec, &actuals_vec);
    
    // è®¡ç®—MSEå’ŒRMSEç”¨äºæ—¥å¿—æ˜¾ç¤º
    let diff = y_pred.sub(&y_test).map_err(|e| format!("è®¡ç®—MSEå¤±è´¥: {}", e))?;
    let squared_diff = diff.sqr().map_err(|e| format!("è®¡ç®—å¹³æ–¹å¤±è´¥: {}", e))?;
    let mse = squared_diff.mean_all().map_err(|e| format!("è®¡ç®—å‡å€¼å¤±è´¥: {}", e))?;
    let mse = mse.to_scalar::<f32>().unwrap() as f64;
    let rmse = mse.sqrt();
    
    println!("è¯„ä¼°ç»“æœ: MSE = {:.4}, RMSE = {:.4}", mse, rmse);
    println!("ğŸ¯ æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡: {:.2}% | ç»¼åˆå‡†ç¡®ç‡: {:.2}%", 
             direction_accuracy * 100.0, combined_accuracy * 100.0);
    println!("ğŸ“Š é¢„æµ‹å¼ é‡ç»´åº¦: {:?}, å®é™…å¼ é‡ç»´åº¦: {:?}", y_pred.dims(), y_test.dims());
    
    // ä¿å­˜æ¨¡å‹
    let model_path = get_model_file_path(&model_id);
    fs::create_dir_all(model_path.parent().unwrap()).map_err(|e| format!("åˆ›å»ºæ¨¡å‹ç›®å½•å¤±è´¥: {}", e))?;
    save_model(&varmap, &model_path).map_err(|e| format!("æ¨¡å‹ä¿å­˜å¤±è´¥: {}", e))?;
    
    // ä¿å­˜æ¨¡å‹å…ƒæ•°æ®
    let metadata = ModelInfo {
        id: model_id,
        name: request.model_name,
        stock_code: request.stock_code,
        created_at: get_current_timestamp(),
        model_type,
        features: request.features,
        target: request.target,
        prediction_days: request.prediction_days,
        accuracy: combined_accuracy,
    };
    
    save_model_metadata(&metadata).map_err(|e| format!("å…ƒæ•°æ®ä¿å­˜å¤±è´¥: {}", e))?;
    
    Ok(TrainingResult {
        metadata,
        accuracy: combined_accuracy,
    })
} 